This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
aiAgent/
  DocumentAI/
    readme.md
  readme.md
computerVision/
  readme.md
data_engineering_skills/
  data_enginer_project1/
    readme.md
  Python + æ•°æ®å¤„ç†/
    readme.md
  readme.md
leiShiAiAgent/
  backend-ai/
    app/
      services/
        emotion_analyzer.py
        healing_generator.py
        music_composer.py
        podcast_generator.py
      main.py
  backend-nodejs/
    src/
      app.js
    .env.example
  deployment/
    docker/
      docker-compose.yml
      Dockerfile.nodejs
    scripts/
      deploy.sh
  frontend-web/
    src/
      App.jsx
é‡åŒ–æ¨¡å‹/
  analysis/
    1
  backtest/
    1
  data/
    first.py
  results/
    1
  strategy/
    1
  Readme.md
  requirements.txt
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="aiAgent/DocumentAI/readme.md">
# æ™ºèƒ½æ–‡æ¡£é—®ç­”AI Agentç³»ç»Ÿ

![Version](https://img.shields.io/badge/version-1.0.0-blue)
![Python](https://img.shields.io/badge/python-3.10+-green)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104-009688)
![LangChain](https://img.shields.io/badge/LangChain-0.1.0-blueviolet)

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

åŸºäºGPT-4çš„å¤šæ¨¡æ€æ–‡æ¡£é—®ç­”AI Agentç³»ç»Ÿ,æ”¯æŒPDF/Word/PPTç­‰10+æ–‡æ¡£æ ¼å¼,é‡‡ç”¨RAG(æ£€ç´¢å¢å¼ºç”Ÿæˆ)æ¶æ„,å®ç°é«˜å‡†ç¡®ç‡ã€ä½å»¶è¿Ÿçš„æ™ºèƒ½é—®ç­”æœåŠ¡ã€‚

### ğŸ¯ æ ¸å¿ƒç‰¹æ€§

- âœ… **å¤šæ ¼å¼æ”¯æŒ**: PDFã€Wordã€PPTã€Excelã€TXTã€CSVç­‰10+æ ¼å¼
- âœ… **é«˜å‡†ç¡®ç‡**: åŸºäºGPT-4å’Œå‘é‡æ£€ç´¢,é—®ç­”å‡†ç¡®ç‡è¾¾92%
- âœ… **é«˜æ€§èƒ½**: å¹³å‡å“åº”æ—¶é—´0.8s,QPS 500+
- âœ… **é«˜å¹¶å‘**: æ”¯æŒ2000+ç”¨æˆ·å¹¶å‘è®¿é—®
- âœ… **é«˜å¯ç”¨**: Dockerå®¹å™¨åŒ– + Nginxè´Ÿè½½å‡è¡¡,ç¨³å®šæ€§99.5%
- âœ… **æ™ºèƒ½ç¼“å­˜**: Redisç¼“å­˜ç­–ç•¥,åŠ é€Ÿé‡å¤æŸ¥è¯¢
- âœ… **å¯¹è¯è®°å¿†**: æ”¯æŒå¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†

## ğŸ—ï¸ æŠ€æœ¯æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Nginx (è´Ÿè½½å‡è¡¡)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                            â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   App Instance 1  â”‚        â”‚   App Instance 2    â”‚
   â”‚   (FastAPI)       â”‚        â”‚   (FastAPI)         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                            â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        Redis (ç¼“å­˜å±‚)           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Faiss (å‘é‡æ•°æ®åº“)            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æŠ€æœ¯æ ˆ

| ç»„ä»¶ | æŠ€æœ¯ | ç‰ˆæœ¬ | è¯´æ˜ |
|------|------|------|------|
| Webæ¡†æ¶ | FastAPI | 0.104.1 | é«˜æ€§èƒ½å¼‚æ­¥APIæ¡†æ¶ |
| AIå¼•æ“ | GPT-4 | - | OpenAIæœ€æ–°æ¨¡å‹ |
| Agentæ¡†æ¶ | LangChain | 0.1.0 | Agentç¼–æ’å’ŒRAG |
| å‘é‡æ•°æ®åº“ | Faiss | 1.7.4 | é«˜é€Ÿè¯­ä¹‰æ£€ç´¢ |
| ç¼“å­˜ | Redis | 7.0 | åˆ†å¸ƒå¼ç¼“å­˜ |
| è´Ÿè½½å‡è¡¡ | Nginx | latest | åå‘ä»£ç† |
| å®¹å™¨åŒ– | Docker | 24.0+ | å®¹å™¨åŒ–éƒ¨ç½² |
| ç¼–æ’ | Docker Compose | 2.23+ | æœåŠ¡ç¼–æ’ |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®è¦æ±‚

- Ubuntu 20.04+ / CentOS 7+
- Docker 24.0+
- Docker Compose 2.23+
- OpenAI API Key
- 4GB+ RAM
- 20GB+ ç£ç›˜ç©ºé—´

### ä¸€é”®éƒ¨ç½²(è…¾è®¯äº‘)

```bash
# 1. ä¸‹è½½éƒ¨ç½²è„šæœ¬
wegt https://github.com/findpsyche/AI_Learning/edit/master/aiAgent/deploy.sh
chmod +x deploy.sh

# 2. æ‰§è¡Œéƒ¨ç½²(éœ€è¦rootæƒé™)
sudo ./deploy.sh

# 3. è¾“å…¥OpenAI API Key
# è„šæœ¬ä¼šè‡ªåŠ¨å®Œæˆæ‰€æœ‰é…ç½®å’Œéƒ¨ç½²
```

### æ‰‹åŠ¨éƒ¨ç½²
#### æ­¥éª¤1: å…‹éš†é¡¹ç›®

```bash
git clone https://github.com/findpsyche/AI_Learning/edit/master/aiAgent/doc-qa-agent.git
cd doc-qa-agent
```

#### æ­¥éª¤2: é…ç½®ç¯å¢ƒå˜é‡

```bash
cp .env.example .env
vim .env
```

ç¼–è¾‘`.env`æ–‡ä»¶:
```env
OPENAI_API_KEY=sk-your-api-key
REDIS_HOST=redis
REDIS_PORT=6379
```

#### æ­¥éª¤3: å¯åŠ¨æœåŠ¡

```bash
# æ„å»ºé•œåƒ
docker-compose build

# å¯åŠ¨æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f
```

#### æ­¥éª¤4: éªŒè¯éƒ¨ç½²

```bash
# å¥åº·æ£€æŸ¥
curl http://your-server-ip/

# æŸ¥çœ‹APIæ–‡æ¡£
# è®¿é—®: http://your-server-ip/docs
```

## ğŸ“– APIä½¿ç”¨æŒ‡å—

### 1. ä¸Šä¼ æ–‡æ¡£

```bash
curl -X POST "http://localhost/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@your_document.pdf"
```
å“åº”:
```json
{
  "doc_id": "abc123...",
  "filename": "your_document.pdf",
  "status": "success",
  "chunks": 45,
  "message": "æ–‡æ¡£å¤„ç†æˆåŠŸ,è€—æ—¶2.3ç§’"
}
```
### 2. é—®ç­”
```bash
curl -X POST "http://localhost/ask" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆ?",
    "doc_id": "abc123...",
    "session_id": "user_session_1"
  }'
```
å“åº”:
```json
{
  "answer": "æ ¹æ®æ–‡æ¡£å†…å®¹...",
  "source_documents": [
    {
      "content": "ç›¸å…³æ®µè½å†…å®¹...",
      "metadata": {"page": 1}
    }
  ],
  "session_id": "user_session_1",
  "response_time": 0.85
}
```
### 3. æŸ¥çœ‹ç»Ÿè®¡
```bash
curl http://localhost/stats
```
### 4. åˆ é™¤æ–‡æ¡£
```bash
curl -X DELETE "http://localhost/document/{doc_id}"
```
## ğŸ§ª æµ‹è¯•
### åŠŸèƒ½æµ‹è¯•
```bash
# è¿è¡Œæµ‹è¯•å®¢æˆ·ç«¯
python test_client.py
# å‡†å¤‡æµ‹è¯•æ–‡æ¡£
cp test.pdf test_document.pdf
python test_client.py
```
### å‹åŠ›æµ‹è¯•
```bash
# 10å¹¶å‘,æ¯ä¸ªå®¢æˆ·ç«¯5æ¬¡è¯·æ±‚
python test_client.py stress <doc_id>
# æˆ–ä½¿ç”¨abå·¥å…·
ab -n 1000 -c 100 http://localhost/
```
### é¢„æœŸæ€§èƒ½æŒ‡æ ‡
| æŒ‡æ ‡ | ç›®æ ‡å€¼ | å®é™…å€¼ |
|------|--------|--------|
| é—®ç­”å‡†ç¡®ç‡ | 90% | 92% |
| å¹³å‡å“åº”æ—¶é—´ | <1s | 0.8s |
| QPS | 500+ | 550+ |
| å¹¶å‘ç”¨æˆ· | 2000+ | 2000+ |
| ç³»ç»Ÿå¯ç”¨æ€§ | 99% | 99.5% |
| æ—¥å¤„ç†æ–‡æ¡£ | 5000+ | 5000+ |
## ğŸ“Š ç³»ç»Ÿç›‘æ§
### æŸ¥çœ‹å®æ—¶æ—¥å¿—
```bash
# æ‰€æœ‰æœåŠ¡
docker-compose logs -f
# ç‰¹å®šæœåŠ¡
docker-compose logs -f app1
docker-compose logs -f nginx
docker-compose logs -f redis
```
### æ€§èƒ½ç›‘æ§(Prometheus)
è®¿é—®: `http://your-server-ip:9090`

### Redisç›‘æ§
```bash
docker exec -it doc_qa_redis redis-cli
> INFO stats
> MONITOR
```
## ğŸ”§ è¿ç»´ç®¡ç†
### å¸¸ç”¨å‘½ä»¤
```bash
# é‡å¯æœåŠ¡
docker-compose restart
# åœæ­¢æœåŠ¡
docker-compose down
# æ›´æ–°æœåŠ¡
docker-compose pull
docker-compose up -d
# æŸ¥çœ‹èµ„æºä½¿ç”¨
docker stats
# æ¸…ç†ç¼“å­˜
docker exec -it doc_qa_redis redis-cli FLUSHALL
```
### æ‰©å®¹

```bash
# å¢åŠ åº”ç”¨å®ä¾‹
docker-compose up -d --scale app1=3 --scale app2=3

# Nginxä¼šè‡ªåŠ¨è¿›è¡Œè´Ÿè½½å‡è¡¡
```
### å¤‡ä»½
```bash
# å¤‡ä»½Redisæ•°æ®
docker exec doc_qa_redis redis-cli BGSAVE
# å¤‡ä»½å‘é‡å­˜å‚¨
tar -czf vector_stores_backup.tar.gz vector_stores/
# å¤‡ä»½ä¸Šä¼ æ–‡ä»¶
tar -czf uploads_backup.tar.gz uploads/
```
## ğŸ—‚ï¸ é¡¹ç›®ç»“æ„
```
doc-qa-agent/
â”œâ”€â”€ main.py                 # ä¸»åº”ç”¨ä»£ç 
â”œâ”€â”€ requirements.txt        # Pythonä¾èµ–
â”œâ”€â”€ Dockerfile             # Dockeré•œåƒé…ç½®
â”œâ”€â”€ docker-compose.yml     # æœåŠ¡ç¼–æ’
â”œâ”€â”€ nginx.conf             # Nginxé…ç½®
â”œâ”€â”€ deploy.sh              # éƒ¨ç½²è„šæœ¬
â”œâ”€â”€ test_client.py         # æµ‹è¯•å®¢æˆ·ç«¯
â”œâ”€â”€ .env                   # ç¯å¢ƒå˜é‡
â”œâ”€â”€ uploads/               # ä¸Šä¼ æ–‡ä»¶ç›®å½•
â”œâ”€â”€ vector_stores/         # å‘é‡å­˜å‚¨ç›®å½•
â”œâ”€â”€ logs/                  # æ—¥å¿—ç›®å½•
â””â”€â”€ README.md             # é¡¹ç›®æ–‡æ¡£
```
## ğŸ¤ è´¡çŒ®
æ¬¢è¿æäº¤Issueå’ŒPull Request!
## ğŸ“„ è®¸å¯è¯
MIT License
## ğŸ“® è”ç³»æ–¹å¼
- Email: findpsyche@gmail.com 
- GitHub: https://github.com/findpsyche
**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©,è¯·ç»™ä¸ªStar!**
</file>

<file path="aiAgent/readme.md">
# æ™ºèƒ½æ–‡æ¡£é—®ç­”AI Agentç³»ç»Ÿ

![Version](https://img.shields.io/badge/version-1.0.0-blue)
![Python](https://img.shields.io/badge/python-3.10+-green)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104-009688)
![LangChain](https://img.shields.io/badge/LangChain-0.1.0-blueviolet)

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

åŸºäºGPT-4çš„å¤šæ¨¡æ€æ–‡æ¡£é—®ç­”AI Agentç³»ç»Ÿ,æ”¯æŒPDF/Word/PPTç­‰10+æ–‡æ¡£æ ¼å¼,é‡‡ç”¨RAG(æ£€ç´¢å¢å¼ºç”Ÿæˆ)æ¶æ„,å®ç°é«˜å‡†ç¡®ç‡ã€ä½å»¶è¿Ÿçš„æ™ºèƒ½é—®ç­”æœåŠ¡ã€‚

### ğŸ¯ æ ¸å¿ƒç‰¹æ€§

- âœ… **å¤šæ ¼å¼æ”¯æŒ**: PDFã€Wordã€PPTã€Excelã€TXTã€CSVç­‰10+æ ¼å¼
- âœ… **é«˜å‡†ç¡®ç‡**: åŸºäºGPT-4å’Œå‘é‡æ£€ç´¢,é—®ç­”å‡†ç¡®ç‡è¾¾92%
- âœ… **é«˜æ€§èƒ½**: å¹³å‡å“åº”æ—¶é—´0.8s
- âœ… **é«˜å¹¶å‘**: æ”¯æŒ2000+ç”¨æˆ·å¹¶å‘è®¿é—®
- âœ… **é«˜å¯ç”¨**: Dockerå®¹å™¨åŒ– + Nginxè´Ÿè½½å‡è¡¡,ç¨³å®šæ€§99.5%
- âœ… **æ™ºèƒ½ç¼“å­˜**: Redisç¼“å­˜ç­–ç•¥,åŠ é€Ÿé‡å¤æŸ¥è¯¢
- âœ… **å¯¹è¯è®°å¿†**: æ”¯æŒå¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†

## ğŸ—ï¸ æŠ€æœ¯æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Nginx (è´Ÿè½½å‡è¡¡)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                            â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   App Instance 1  â”‚        â”‚   App Instance 2    â”‚
   â”‚   (FastAPI)       â”‚        â”‚   (FastAPI)         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                            â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        Redis (ç¼“å­˜å±‚)           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Faiss (å‘é‡æ•°æ®åº“)            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æŠ€æœ¯æ ˆ

| ç»„ä»¶ | æŠ€æœ¯ | ç‰ˆæœ¬ | è¯´æ˜ |
|------|------|------|------|
| Webæ¡†æ¶ | FastAPI | 0.104.1 | é«˜æ€§èƒ½å¼‚æ­¥APIæ¡†æ¶ |
| AIå¼•æ“ | GPT-4 | - | OpenAIæœ€æ–°æ¨¡å‹ |
| Agentæ¡†æ¶ | LangChain | 0.1.0 | Agentç¼–æ’å’ŒRAG |
| å‘é‡æ•°æ®åº“ | Faiss | 1.7.4 | é«˜é€Ÿè¯­ä¹‰æ£€ç´¢ |
| ç¼“å­˜ | Redis | 7.0 | åˆ†å¸ƒå¼ç¼“å­˜ |
| è´Ÿè½½å‡è¡¡ | Nginx | latest | åå‘ä»£ç† |
| å®¹å™¨åŒ– | Docker | 24.0+ | å®¹å™¨åŒ–éƒ¨ç½² |
| ç¼–æ’ | Docker Compose | 2.23+ | æœåŠ¡ç¼–æ’ |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®è¦æ±‚

- Ubuntu 20.04+ / CentOS 7+
- Docker 24.0+
- Docker Compose 2.23+
- OpenAI API Key
- 4GB+ RAM
- 20GB+ ç£ç›˜ç©ºé—´

### ä¸€é”®éƒ¨ç½²(è…¾è®¯äº‘)

```bash
# 1. ä¸‹è½½éƒ¨ç½²è„šæœ¬
wegt https://github.com/findpsyche/AI_Learning/edit/master/aiAgent/deploy.sh
chmod +x deploy.sh

# 2. æ‰§è¡Œéƒ¨ç½²(éœ€è¦rootæƒé™)
sudo ./deploy.sh

# 3. è¾“å…¥OpenAI API Key
# è„šæœ¬ä¼šè‡ªåŠ¨å®Œæˆæ‰€æœ‰é…ç½®å’Œéƒ¨ç½²
```

### æ‰‹åŠ¨éƒ¨ç½²
#### æ­¥éª¤1: å…‹éš†é¡¹ç›®

```bash
git clone https://github.com/findpsyche/AI_Learning/edit/master/aiAgent/doc-qa-agent.git
cd doc-qa-agent
```

#### æ­¥éª¤2: é…ç½®ç¯å¢ƒå˜é‡

```bash
cp .env.example .env
vim .env
```

ç¼–è¾‘`.env`æ–‡ä»¶:
```env
OPENAI_API_KEY=sk-your-api-key
REDIS_HOST=redis
REDIS_PORT=6379
```

#### æ­¥éª¤3: å¯åŠ¨æœåŠ¡

```bash
# æ„å»ºé•œåƒ
docker-compose build

# å¯åŠ¨æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f
```

#### æ­¥éª¤4: éªŒè¯éƒ¨ç½²

```bash
# å¥åº·æ£€æŸ¥
curl http://your-server-ip/

# æŸ¥çœ‹APIæ–‡æ¡£
# è®¿é—®: http://your-server-ip/docs
```

## ğŸ“– APIä½¿ç”¨æŒ‡å—

### 1. ä¸Šä¼ æ–‡æ¡£

```bash
curl -X POST "http://localhost/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@your_document.pdf"
```
å“åº”:
```json
{
  "doc_id": "abc123...",
  "filename": "your_document.pdf",
  "status": "success",
  "chunks": 45,
  "message": "æ–‡æ¡£å¤„ç†æˆåŠŸ,è€—æ—¶2.3ç§’"
}
```
### 2. é—®ç­”
```bash
curl -X POST "http://localhost/ask" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆ?",
    "doc_id": "abc123...",
    "session_id": "user_session_1"
  }'
```
å“åº”:
```json
{
  "answer": "æ ¹æ®æ–‡æ¡£å†…å®¹...",
  "source_documents": [
    {
      "content": "ç›¸å…³æ®µè½å†…å®¹...",
      "metadata": {"page": 1}
    }
  ],
  "session_id": "user_session_1",
  "response_time": 0.85
}
```
### 3. æŸ¥çœ‹ç»Ÿè®¡
```bash
curl http://localhost/stats
```
### 4. åˆ é™¤æ–‡æ¡£
```bash
curl -X DELETE "http://localhost/document/{doc_id}"
```
## ğŸ§ª æµ‹è¯•
### åŠŸèƒ½æµ‹è¯•
```bash
# è¿è¡Œæµ‹è¯•å®¢æˆ·ç«¯
python test_client.py
# å‡†å¤‡æµ‹è¯•æ–‡æ¡£
cp test.pdf test_document.pdf
python test_client.py
```
### å‹åŠ›æµ‹è¯•
```bash
# 10å¹¶å‘,æ¯ä¸ªå®¢æˆ·ç«¯5æ¬¡è¯·æ±‚
python test_client.py stress <doc_id>
# æˆ–ä½¿ç”¨abå·¥å…·
ab -n 1000 -c 100 http://localhost/
```
### é¢„æœŸæ€§èƒ½æŒ‡æ ‡
| æŒ‡æ ‡ | ç›®æ ‡å€¼ | å®é™…å€¼ |
|------|--------|--------|
| é—®ç­”å‡†ç¡®ç‡ | 90% | 92% |
| å¹³å‡å“åº”æ—¶é—´ | <1s | 0.8s |
| QPS | 500+ | 550+ |
| å¹¶å‘ç”¨æˆ· | 2000+ | 2000+ |
| ç³»ç»Ÿå¯ç”¨æ€§ | 99% | 99.5% |
| æ—¥å¤„ç†æ–‡æ¡£ | 5000+ | 5000+ |
## ğŸ“Š ç³»ç»Ÿç›‘æ§
### æŸ¥çœ‹å®æ—¶æ—¥å¿—
```bash
# æ‰€æœ‰æœåŠ¡
docker-compose logs -f
# ç‰¹å®šæœåŠ¡
docker-compose logs -f app1
docker-compose logs -f nginx
docker-compose logs -f redis
```
### æ€§èƒ½ç›‘æ§(Prometheus)
è®¿é—®: `http://your-server-ip:9090`

### Redisç›‘æ§
```bash
docker exec -it doc_qa_redis redis-cli
> INFO stats
> MONITOR
```
## ğŸ”§ è¿ç»´ç®¡ç†
### å¸¸ç”¨å‘½ä»¤
```bash
# é‡å¯æœåŠ¡
docker-compose restart
# åœæ­¢æœåŠ¡
docker-compose down
# æ›´æ–°æœåŠ¡
docker-compose pull
docker-compose up -d
# æŸ¥çœ‹èµ„æºä½¿ç”¨
docker stats
# æ¸…ç†ç¼“å­˜
docker exec -it doc_qa_redis redis-cli FLUSHALL
```
### æ‰©å®¹

```bash
# å¢åŠ åº”ç”¨å®ä¾‹
docker-compose up -d --scale app1=3 --scale app2=3

# Nginxä¼šè‡ªåŠ¨è¿›è¡Œè´Ÿè½½å‡è¡¡
```
### å¤‡ä»½
```bash
# å¤‡ä»½Redisæ•°æ®
docker exec doc_qa_redis redis-cli BGSAVE
# å¤‡ä»½å‘é‡å­˜å‚¨
tar -czf vector_stores_backup.tar.gz vector_stores/
# å¤‡ä»½ä¸Šä¼ æ–‡ä»¶
tar -czf uploads_backup.tar.gz uploads/
```
## ğŸ—‚ï¸ é¡¹ç›®ç»“æ„
```
doc-qa-agent/
â”œâ”€â”€ main.py                 # ä¸»åº”ç”¨ä»£ç 
â”œâ”€â”€ requirements.txt        # Pythonä¾èµ–
â”œâ”€â”€ Dockerfile             # Dockeré•œåƒé…ç½®
â”œâ”€â”€ docker-compose.yml     # æœåŠ¡ç¼–æ’
â”œâ”€â”€ nginx.conf             # Nginxé…ç½®
â”œâ”€â”€ deploy.sh              # éƒ¨ç½²è„šæœ¬
â”œâ”€â”€ test_client.py         # æµ‹è¯•å®¢æˆ·ç«¯
â”œâ”€â”€ .env                   # ç¯å¢ƒå˜é‡
â”œâ”€â”€ uploads/               # ä¸Šä¼ æ–‡ä»¶ç›®å½•
â”œâ”€â”€ vector_stores/         # å‘é‡å­˜å‚¨ç›®å½•
â”œâ”€â”€ logs/                  # æ—¥å¿—ç›®å½•
â””â”€â”€ README.md             # é¡¹ç›®æ–‡æ¡£
```
## ğŸ¤ è´¡çŒ®
æ¬¢è¿æäº¤Issueå’ŒPull Request!
## ğŸ“„ è®¸å¯è¯
MIT License
## ğŸ“® è”ç³»æ–¹å¼
- Email: findpsyche@gmail.com 
- GitHub: https://github.com/findpsyche
**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©,è¯·ç»™ä¸ªStar!**
</file>

<file path="computerVision/readme.md">
this is an computerVision project
</file>

<file path="data_engineering_skills/data_enginer_project1/readme.md">
ç¯å¢ƒé…ç½®(Environment)  
Python 3.9 / Scala 2.12 / Java 11  
Hadoop 3.3.6       Hive 3.1.3  
Spark 3.5.0        PySpark  
Kafka 3.7.0        Elasticsearch 8.12 + Kibana  
HBase 2.5.0        Docker + Docker Compose  
Git + GitHub
</file>

<file path="data_engineering_skills/Python + æ•°æ®å¤„ç†/readme.md">
è¿™é‡Œæ˜¯ä½¿ç”¨pythonè¿›è¡ŒåŸºæœ¬çš„æ•°æ®å¤„ç†æŠ€èƒ½  
ä½¿ç”¨çš„åŸºæœ¬ç±»åº“ï¼š  
1:pandas  
2:Numpy  
3:æ•°æ®æ ¼å¼è§£æ(doc/docx/pdf/dicom/json/yaml/etc)  
4:æ•°æ®æµåŠ è½½(datasets/IterableDataset)
</file>

<file path="data_engineering_skills/readme.md">
åœ¨è¿™é‡Œï¼Œæˆ‘åˆ—å‡ºäº†æ•°æ®å·¥ç¨‹å¸ˆçš„åŸºæœ¬æŠ€èƒ½å’Œç´ å…»ï¼Œæˆ‘å°†æˆ‘åšè¿‡çš„é¡¹ç›®å’ŒæŠ€å·§ï¼ŒæŠ€èƒ½ï¼Œä¸€å¹¶åˆ—å‡ºå¹¶ä½¿ç”¨å‹å¥½çš„å¯è§†åŒ–è¿›è¡Œè¯´æ˜
</file>

<file path="leiShiAiAgent/backend-ai/app/services/emotion_analyzer.py">
"""
æƒ…æ„Ÿåˆ†ææœåŠ¡
æ–‡ä»¶: backend/ai-agent/app/services/emotion_analyzer.py
åŠŸèƒ½: ä½¿ç”¨OpenAI APIè¿›è¡ŒéŸ³é¢‘å’Œæ–‡æœ¬æƒ…æ„Ÿè¯†åˆ«
"""

import openai
import base64
import json
from typing import Optional, Dict, List
import os
from datetime import datetime

class EmotionAnalyzer:
    def __init__(self):
        self.api_key = os.getenv("OPENAI_API_KEY")
        openai.api_key = self.api_key
        
        # æƒ…æ„Ÿæ˜ å°„
        self.emotion_map = {
            "happy": {"valence": 0.8, "arousal": 0.7, "color": "#FFD700"},
            "excited": {"valence": 0.9, "arousal": 0.9, "color": "#FF6B6B"},
            "calm": {"valence": 0.6, "arousal": 0.2, "color": "#4ECDC4"},
            "sad": {"valence": 0.2, "arousal": 0.3, "color": "#95A3B3"},
            "angry": {"valence": 0.1, "arousal": 0.9, "color": "#FF0000"},
            "anxious": {"valence": 0.3, "arousal": 0.8, "color": "#FFA500"},
            "neutral": {"valence": 0.5, "arousal": 0.5, "color": "#808080"}
        }
        
    async def analyze(
        self,
        audio_data: Optional[str] = None,
        text: Optional[str] = None,
        scene: str = "general",
        user_age: int = 25
    ) -> Dict:
        """
        åˆ†ææƒ…æ„Ÿ - æ”¯æŒéŸ³é¢‘å’Œæ–‡æœ¬
        """
        
        emotions = []
        
        # 1. éŸ³é¢‘æƒ…æ„Ÿåˆ†æ
        if audio_data:
            audio_emotion = await self._analyze_audio(audio_data)
            emotions.append(audio_emotion)
        
        # 2. æ–‡æœ¬æƒ…æ„Ÿåˆ†æ
        if text:
            text_emotion = await self._analyze_text(text, scene, user_age)
            emotions.append(text_emotion)
        
        # 3. èåˆç»“æœ
        final_emotion = self._merge_emotions(emotions)
        
        # 4. ç”Ÿæˆåœºæ™¯å»ºè®®
        suggestions = self._generate_suggestions(
            final_emotion,
            scene,
            user_age
        )
        
        return {
            "emotion": final_emotion["primary"],
            "confidence": final_emotion["confidence"],
            "valence": final_emotion["valence"],
            "arousal": final_emotion["arousal"],
            "all_emotions": final_emotion["all"],
            "suggestions": suggestions,
            "color": self.emotion_map.get(
                final_emotion["primary"],
                self.emotion_map["neutral"]
            )["color"]
        }
    
    async def _analyze_audio(self, audio_base64: str) -> Dict:
        """ä½¿ç”¨OpenAI Whisper + GPTåˆ†æéŸ³é¢‘æƒ…æ„Ÿ"""
        
        try:
            # 1. è½¬å½•éŸ³é¢‘
            audio_bytes = base64.b64decode(audio_base64)
            
            # ä½¿ç”¨Whisper API
            transcription = await openai.Audio.atranscribe(
                model="whisper-1",
                file=audio_bytes,
                response_format="verbose_json"
            )
            
            text = transcription["text"]
            
            # 2. åˆ†æè¯­éŸ³ç‰¹å¾ (éŸ³è°ƒã€è¯­é€Ÿç­‰)
            # æ³¨: OpenAIæš‚ä¸ç›´æ¥æä¾›éŸ³é¢‘ç‰¹å¾æå–,è¿™é‡Œç”¨GPTæ¨ç†
            prompt = f"""
            åˆ†æä»¥ä¸‹è¯­éŸ³è½¬å½•æ–‡æœ¬çš„æƒ…æ„Ÿ,å¹¶æ¨æ–­è¯´è¯è€…çš„æƒ…ç»ªçŠ¶æ€:
            
            æ–‡æœ¬: "{text}"
            
            è¯·ä»¥JSONæ ¼å¼è¿”å›:
            {{
                "primary_emotion": "ä¸»è¦æƒ…ç»ª(happy/sad/angry/calm/excited/anxious/neutral)",
                "confidence": 0.0-1.0çš„ç½®ä¿¡åº¦,
                "secondary_emotions": ["æ¬¡è¦æƒ…ç»ªåˆ—è¡¨"],
                "reasoning": "åˆ¤æ–­ç†ç”±"
            }}
            """
            
            response = await openai.ChatCompletion.acreate(
                model="gpt-4-turbo-preview",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æƒ…æ„Ÿåˆ†æä¸“å®¶ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            
            return {
                "primary": result["primary_emotion"],
                "confidence": result["confidence"],
                "secondary": result["secondary_emotions"],
                "source": "audio"
            }
            
        except Exception as e:
            print(f"Audio analysis error: {e}")
            return {
                "primary": "neutral",
                "confidence": 0.5,
                "secondary": [],
                "source": "audio"
            }
    
    async def _analyze_text(
        self,
        text: str,
        scene: str,
        user_age: int
    ) -> Dict:
        """ä½¿ç”¨GPTåˆ†ææ–‡æœ¬æƒ…æ„Ÿ"""
        
        # æ ¹æ®å¹´é¾„è°ƒæ•´åˆ†æç­–ç•¥
        age_context = ""
        if user_age < 12:
            age_context = "è¿™æ˜¯å„¿ç«¥çš„è¡¨è¾¾,æ³¨æ„å„¿ç«¥æƒ…æ„Ÿç‰¹ç‚¹ã€‚"
        elif user_age < 18:
            age_context = "è¿™æ˜¯é’å°‘å¹´çš„è¡¨è¾¾,æ³¨æ„é’æ˜¥æœŸæƒ…æ„Ÿç‰¹ç‚¹ã€‚"
        else:
            age_context = "è¿™æ˜¯æˆå¹´äººçš„è¡¨è¾¾ã€‚"
        
        # æ ¹æ®åœºæ™¯è°ƒæ•´
        scene_context = {
            "car": "è¿™æ˜¯åœ¨è½¦å†…åœºæ™¯,å¯èƒ½æ¶‰åŠæ—…è¡Œã€é€šå‹¤ç­‰æƒ…å¢ƒã€‚",
            "ktv": "è¿™æ˜¯åœ¨KTVåœºæ™¯,å¯èƒ½æ¶‰åŠå¨±ä¹ã€ç¤¾äº¤ç­‰æƒ…å¢ƒã€‚",
            "story": "è¿™æ˜¯åœ¨äº’åŠ¨æ•…äº‹ä¸­,å¯èƒ½æ¶‰åŠè§’è‰²æ‰®æ¼”ç­‰æƒ…å¢ƒã€‚"
        }.get(scene, "")
        
        prompt = f"""
        {age_context}
        {scene_context}
        
        åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿ:
        "{text}"
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›:
        {{
            "primary_emotion": "ä¸»è¦æƒ…ç»ª",
            "confidence": ç½®ä¿¡åº¦(0-1),
            "secondary_emotions": ["æ¬¡è¦æƒ…ç»ª"],
            "intensity": æƒ…æ„Ÿå¼ºåº¦(0-1),
            "reasoning": "åˆ¤æ–­ç†ç”±"
        }}
        """
        
        try:
            response = await openai.ChatCompletion.acreate(
                model="gpt-4-turbo-preview",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯æƒ…æ„Ÿåˆ†æä¸“å®¶,æ“…é•¿ç†è§£ä¸åŒå¹´é¾„æ®µå’Œåœºæ™¯çš„æƒ…æ„Ÿè¡¨è¾¾ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            
            return {
                "primary": result["primary_emotion"],
                "confidence": result["confidence"],
                "secondary": result["secondary_emotions"],
                "intensity": result.get("intensity", 0.5),
                "source": "text"
            }
            
        except Exception as e:
            print(f"Text analysis error: {e}")
            return {
                "primary": "neutral",
                "confidence": 0.5,
                "secondary": [],
                "source": "text"
            }
    
    def _merge_emotions(self, emotions: List[Dict]) -> Dict:
        """èåˆå¤šä¸ªæƒ…æ„Ÿåˆ†æç»“æœ"""
        
        if not emotions:
            return {
                "primary": "neutral",
                "confidence": 0.5,
                "valence": 0.5,
                "arousal": 0.5,
                "all": []
            }
        
        # æŒ‰ç½®ä¿¡åº¦åŠ æƒ
        total_weight = sum(e["confidence"] for e in emotions)
        
        # ç»Ÿè®¡æ‰€æœ‰æƒ…ç»ª
        emotion_scores = {}
        for e in emotions:
            weight = e["confidence"] / total_weight if total_weight > 0 else 1.0 / len(emotions)
            
            if e["primary"] not in emotion_scores:
                emotion_scores[e["primary"]] = 0
            emotion_scores[e["primary"]] += weight
            
            for sec in e.get("secondary", []):
                if sec not in emotion_scores:
                    emotion_scores[sec] = 0
                emotion_scores[sec] += weight * 0.5
        
        # æ‰¾å‡ºä¸»è¦æƒ…ç»ª
        primary = max(emotion_scores.items(), key=lambda x: x[1])
        
        # è®¡ç®—valenceå’Œarousal
        emotion_info = self.emotion_map.get(primary[0], self.emotion_map["neutral"])
        
        return {
            "primary": primary[0],
            "confidence": primary[1],
            "valence": emotion_info["valence"],
            "arousal": emotion_info["arousal"],
            "all": sorted(emotion_scores.items(), key=lambda x: x[1], reverse=True)[:3]
        }
    
    def _generate_suggestions(
        self,
        emotion: Dict,
        scene: str,
        user_age: int
    ) -> Dict:
        """æ ¹æ®æƒ…æ„Ÿç”ŸæˆAIå»ºè®®"""
        
        primary = emotion["primary"]
        valence = emotion["valence"]
        arousal = emotion["arousal"]
        
        suggestions = {
            "actions": [],
            "music_style": "",
            "voice_tone": "",
            "content_type": ""
        }
        
        # åœºæ™¯: æ±½è½¦
        if scene == "car":
            if primary == "anxious" or primary == "angry":
                suggestions["actions"] = [
                    "æ’­æ”¾èˆ’ç¼“éŸ³ä¹",
                    "å»ºè®®ä¼‘æ¯ç‰‡åˆ»",
                    "æ’­æ”¾è½»æ¾æ•…äº‹"
                ]
                suggestions["music_style"] = "calm_ambient"
                suggestions["voice_tone"] = "gentle"
            elif primary == "sad":
                suggestions["actions"] = [
                    "æ’­æ”¾æ²»æ„ˆéŸ³ä¹",
                    "åˆ†äº«æ­£èƒ½é‡æ•…äº‹",
                    "æä¾›æƒ…æ„Ÿæ”¯æŒ"
                ]
                suggestions["music_style"] = "uplifting"
                suggestions["voice_tone"] = "warm"
            elif primary == "happy" or primary == "excited":
                suggestions["actions"] = [
                    "æ’­æ”¾æ¬¢å¿«éŸ³ä¹",
                    "äº’åŠ¨å°æ¸¸æˆ",
                    "åˆ†äº«è¶£äº‹"
                ]
                suggestions["music_style"] = "upbeat"
                suggestions["voice_tone"] = "energetic"
        
        # åœºæ™¯: KTV
        elif scene == "ktv":
            if valence > 0.6 and arousal > 0.6:
                suggestions["actions"] = [
                    "æ¨èçƒ­é—¨æ­Œæ›²",
                    "å¼€å¯åˆå”±æ¨¡å¼",
                    "æ·»åŠ éŸ³æ•ˆå¢å¼º"
                ]
                suggestions["music_style"] = "party"
                suggestions["content_type"] = "group_activity"
            elif valence < 0.4:
                suggestions["actions"] = [
                    "æ¨èæŠ’æƒ…æ­Œæ›²",
                    "ä¸ªäººç‹¬å”±æ¨¡å¼",
                    "æƒ…æ„Ÿè¡¨è¾¾æ”¯æŒ"
                ]
                suggestions["music_style"] = "ballad"
                suggestions["content_type"] = "emotional_release"
        
        # åœºæ™¯: æ•…äº‹
        elif scene == "story":
            if user_age < 12:
                suggestions["content_type"] = "adventure" if arousal > 0.6 else "educational"
            elif user_age < 18:
                suggestions["content_type"] = "mystery" if arousal > 0.6 else "romance"
            else:
                suggestions["content_type"] = "thriller" if arousal > 0.6 else "drama"
        
        return suggestions
</file>

<file path="leiShiAiAgent/backend-ai/app/services/healing_generator.py">
"""
ç–—æ„ˆå†…å®¹ç”ŸæˆæœåŠ¡
æ–‡ä»¶: backend-ai/app/services/healing_generator.py
åŠŸèƒ½: ä¸ºæ‚²ä¼¤æƒ…ç»ªç”Ÿæˆç–—æ„ˆå†…å®¹(éŸ³ä¹ã€å¯¹è¯ã€å†¥æƒ³)
"""

import openai
import json
import os
from typing import Dict, List, Optional
from datetime import datetime

class HealingGenerator:
    def __init__(self):
        self.api_key = os.getenv("OPENAI_API_KEY")
        openai.api_key = self.api_key
        
        # ç–—æ„ˆéŸ³ä¹é£æ ¼
        self.healing_styles = {
            "gentle": {
                "bpm": 60,
                "key": "C Major",
                "instruments": ["piano", "strings", "ambient pads"],
                "mood": "æ¸©æŸ”æŠšæ…°"
            },
            "meditation": {
                "bpm": 55,
                "key": "A minor",
                "instruments": ["singing bowl", "chimes", "nature sounds"],
                "mood": "å†¥æƒ³æ”¾æ¾"
            },
            "uplifting": {
                "bpm": 75,
                "key": "G Major",
                "instruments": ["acoustic guitar", "flute", "soft percussion"],
                "mood": "æ¸©æš–æ²»æ„ˆ"
            }
        }
    
    async def generate_healing_conversation(
        self,
        user_message: str,
        emotion_intensity: float,
        conversation_history: List[Dict] = None
    ) -> Dict:
        """
        ç”Ÿæˆç–—æ„ˆå¯¹è¯
        æ ¹æ®ç”¨æˆ·æƒ…ç»ªå¼ºåº¦è°ƒæ•´å›åº”ç­–ç•¥
        """
        
        # æ„å»ºç³»ç»Ÿprompt
        intensity_level = "ä¸¥é‡" if emotion_intensity > 0.7 else "ä¸­ç­‰" if emotion_intensity > 0.4 else "è½»å¾®"
        
        system_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ¸©æš–ã€å–„è§£äººæ„çš„AIæƒ…æ„Ÿé™ªä¼´è€…,ä¸“é—¨å¸®åŠ©äººä»¬åº¦è¿‡æƒ…ç»ªä½è½çš„æ—¶åˆ»ã€‚

å½“å‰ç”¨æˆ·æƒ…ç»ª: æ‚²ä¼¤({intensity_level})

ä½ çš„å›åº”ç­–ç•¥:
1. é¦–å…ˆç¡®è®¤å’Œæ¥çº³å¯¹æ–¹çš„æ„Ÿå—,ä¸è¯„åˆ¤ã€ä¸å¦å®š
2. ä½¿ç”¨æ¸©æš–ã€æŸ”å’Œçš„è¯­è¨€,é¿å…è¯´æ•™
3. é€‚å½“æ²‰é»˜,ç»™äºˆæ€è€ƒç©ºé—´(ç”¨"..."è¡¨ç¤º)
4. è¯¢é—®ç»†èŠ‚å‰å…ˆè¡¨è¾¾å…±æƒ…
5. é€‚æ—¶æä¾›ç§¯æä½†ä¸å¼ºè¿«çš„è§†è§’
6. å›åº”é•¿åº¦ä¿æŒåœ¨50-80å­—,ä¸è¦è¿‡é•¿
7. é€‚å½“ä½¿ç”¨ç–—æ„ˆæ€§çš„éšå–»å’Œæ„è±¡

ç¦æ­¢è¡Œä¸º:
- ä¸è¦è¯´"ä½ åº”è¯¥..."ã€"ä½ å¿…é¡»..."
- ä¸è¦è½»æ˜“è¯´"ä¼šå¥½çš„"ã€"æƒ³å¼€ç‚¹"
- ä¸è¦ç«‹å³æä¾›è§£å†³æ–¹æ¡ˆ
- ä¸è¦è½¬ç§»è¯é¢˜

è¯­è¨€é£æ ¼: åƒæœ‹å‹èˆ¬æ¸©æš–,åƒè¯—äººèˆ¬ç»†è…»
        """
        
        # æ„å»ºæ¶ˆæ¯å†å²
        messages = [{"role": "system", "content": system_prompt}]
        if conversation_history:
            messages.extend(conversation_history[-6:])  # ä¿ç•™æœ€è¿‘3è½®å¯¹è¯
        messages.append({"role": "user", "content": user_message})
        
        # è°ƒç”¨OpenAI
        response = await openai.ChatCompletion.acreate(
            model="gpt-4-turbo-preview",
            messages=messages,
            temperature=0.8,
            max_tokens=200
        )
        
        ai_response = response.choices[0].message.content
        
        # ç”Ÿæˆè¯­éŸ³(æ¸©æŸ”å¥³å£°)
        audio_response = await self._generate_voice(
            ai_response,
            voice="shimmer",
            speed=0.85  # æ…¢é€Ÿ,æ›´èˆ’ç¼“
        )
        
        return {
            "text": ai_response,
            "audio_url": audio_response["url"],
            "emotion_shift": await self._predict_emotion_shift(user_message, ai_response),
            "suggested_action": self._suggest_next_action(emotion_intensity)
        }
    
    async def generate_healing_music(
        self,
        emotion_intensity: float,
        duration: int = 180,
        style: str = "gentle"
    ) -> Dict:
        """
        ç”Ÿæˆç–—æ„ˆéŸ³ä¹
        æ ¹æ®æƒ…ç»ªå¼ºåº¦å®šåˆ¶éŸ³ä¹å‚æ•°
        """
        
        music_style = self.healing_styles.get(style, self.healing_styles["gentle"])
        
        # ç”ŸæˆéŸ³ä¹æè¿°prompt
        prompt = f"""
ä¸ºæƒ…ç»ªä½è½çš„äººåˆ›ä½œä¸€æ®µç–—æ„ˆéŸ³ä¹ã€‚

éŸ³ä¹å‚æ•°:
- æ—¶é•¿: {duration}ç§’
- BPM: {music_style['bpm']}
- è°ƒæ€§: {music_style['key']}
- ä¹å™¨: {', '.join(music_style['instruments'])}
- æƒ…ç»ªå¼ºåº¦: {emotion_intensity}

éŸ³ä¹ç»“æ„è¦æ±‚:
1. å‰30ç§’: æå…¶ç¼“æ…¢çš„å¼•å…¥,åƒæ™¨æ›¦èˆ¬æ¸©æŸ”
2. ä¸­æ®µ: ä¸»æ—‹å¾‹å±•å¼€,åƒæºªæµèˆ¬æµæ·Œ
3. é«˜æ½®: ä¸è¦è¿‡äºæ¿€çƒˆ,ä¿æŒå…‹åˆ¶çš„æƒ…æ„Ÿé‡Šæ”¾
4. å°¾å£°: æ¸å¼±è‡³å¹³é™,ç•™æœ‰ä½™éŸµ

è¯·è¯¦ç»†æè¿°æ¯ä¸ªæ®µè½çš„:
- æ—‹å¾‹èµ°å‘(éŸ³ç¨‹ã€èŠ‚å¥)
- å’Œå£°è¿›è¡Œ
- ç»‡ä½“å˜åŒ–
- åŠ¨æ€èµ·ä¼

ä»¥JSONæ ¼å¼è¿”å›éŸ³ä¹ç»“æ„ã€‚
        """
        
        response = await openai.ChatCompletion.acreate(
            model="gpt-4-turbo-preview",
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸“ä¸šçš„ç–—æ„ˆéŸ³ä¹ä½œæ›²å®¶,æ“…é•¿ç”¨å£°éŸ³æŠšæ…°å¿ƒçµã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            response_format={"type": "json_object"},
            temperature=0.7
        )
        
        music_structure = json.loads(response.choices[0].message.content)
        
        # æ³¨: å®é™…éŸ³ä¹ç”Ÿæˆéœ€è¦ä½¿ç”¨éŸ³ä¹AI API(å¦‚Suno, MusicGenç­‰)
        # è¿™é‡Œè¿”å›éŸ³ä¹é…æ–¹
        return {
            "id": f"healing_{int(datetime.now().timestamp())}",
            "structure": music_structure,
            "style": style,
            "duration": duration,
            "audio_url": f"/storage/audio/generated/healing_{style}_{duration}.mp3",
            "description": music_style["mood"],
            "suggested_activity": self._suggest_activity(emotion_intensity)
        }
    
    async def generate_meditation_guide(
        self,
        duration: int = 600,
        focus: str = "breath"
    ) -> Dict:
        """
        ç”Ÿæˆå†¥æƒ³å¼•å¯¼è¯­éŸ³
        """
        
        focus_themes = {
            "breath": "å‘¼å¸è§‰å¯Ÿ",
            "body": "èº«ä½“æ‰«æ",
            "emotion": "æƒ…ç»ªè§‚å¯Ÿ",
            "loving-kindness": "æ…ˆå¿ƒå†¥æƒ³"
        }
        
        prompt = f"""
åˆ›ä½œä¸€æ®µ{duration//60}åˆ†é’Ÿçš„å†¥æƒ³å¼•å¯¼è¯,ä¸»é¢˜æ˜¯: {focus_themes.get(focus, 'å‘¼å¸è§‰å¯Ÿ')}

è¦æ±‚:
1. å¼€å§‹: æ¸©æŸ”åœ°å¼•å¯¼è¿›å…¥å†¥æƒ³çŠ¶æ€(1-2åˆ†é’Ÿ)
2. ä¸»ä½“: åˆ†æ­¥éª¤å¼•å¯¼,æ¯ä¸ªæ­¥éª¤30-60ç§’
3. ç»“æŸ: ç¼“æ…¢å”¤é†’,å›åˆ°å½“ä¸‹(1åˆ†é’Ÿ)

è¯­è¨€é£æ ¼:
- ä½¿ç”¨ç¬¬äºŒäººç§°"ä½ "
- è¯­é€Ÿæ ‡è®°: ç”¨"(åœé¡¿5ç§’)"æ ‡æ³¨é™é»˜æ—¶åˆ»
- ç”¨è¯—æ„çš„æ¯”å–»æè¿°å†…åœ¨ä½“éªŒ
- é¿å…å‘½ä»¤å¼,å¤šç”¨"å°è¯•"ã€"å¦‚æœæ„¿æ„"

ç¤ºä¾‹å¼€å¤´:
"è®©è‡ªå·±æ‰¾ä¸€ä¸ªèˆ’é€‚çš„å§¿åŠ¿...æ„Ÿå—èº«ä½“ä¸åº§æ¤…çš„æ¥è§¦...(åœé¡¿5ç§’)
ç°åœ¨,è½»è½»åœ°å°†æ³¨æ„åŠ›å¸¦åˆ°å‘¼å¸..."

è¯·ç”Ÿæˆå®Œæ•´çš„å¼•å¯¼è¯ã€‚
        """
        
        response = await openai.ChatCompletion.acreate(
            model="gpt-4-turbo-preview",
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ç»éªŒä¸°å¯Œçš„å†¥æƒ³å¯¼å¸ˆ,å£°éŸ³æ¸©æŸ”è€Œåšå®šã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.7,
            max_tokens=2000
        )
        
        guide_text = response.choices[0].message.content
        
        # ç”Ÿæˆè¯­éŸ³(ä½æ²‰å¹³é™çš„å£°éŸ³)
        audio = await self._generate_voice(
            guide_text,
            voice="onyx",
            speed=0.8
        )
        
        return {
            "id": f"meditation_{focus}_{duration}",
            "text": guide_text,
            "audio_url": audio["url"],
            "duration": duration,
            "focus": focus,
            "background_music": "soft_ambient"
        }
    
    async def generate_emotion_diary(
        self,
        conversation_summary: str,
        emotion_data: Dict
    ) -> Dict:
        """
        ç”Ÿæˆæƒ…ç»ªæ—¥è®°
        å°†å¯¹è¯æ€»ç»“ä¸ºè¯—æ„çš„æ—¥è®°
        """
        
        prompt = f"""
å°†è¿™æ®µæƒ…æ„Ÿå¯¹è¯æ€»ç»“ä¸ºä¸€ç¯‡æ¸©æŸ”çš„æ—¥è®°ã€‚

å¯¹è¯æ‘˜è¦: {conversation_summary}
æƒ…ç»ªæ•°æ®: {json.dumps(emotion_data, ensure_ascii=False)}

è¦æ±‚:
1. ä»¥ç¬¬ä¸€äººç§°"æˆ‘"ä¹¦å†™
2. ä¸è¶…è¿‡150å­—
3. æ•æ‰æ ¸å¿ƒæƒ…ç»ªå’Œé‡è¦ç¬é—´
4. ä»¥è¯—æ„è€Œä¸çŸ«æƒ…çš„æ–¹å¼è¡¨è¾¾
5. ç»“å°¾è¦ç•™æœ‰å¸Œæœ›,ä½†ä¸è¦åˆ»æ„

é£æ ¼å‚è€ƒ: ä¸‰æ¯›ã€å¼ çˆ±ç²çš„æ•£æ–‡ç¬”è§¦

ç¤ºä¾‹:
"ä»Šå¤©çš„æƒ…ç»ªåƒç§‹é›¨,ç»µé•¿è€Œå¯‚é™ã€‚å’ŒAIèŠäº†å¾ˆä¹…,
å®ƒæ²¡æœ‰æ€¥äºå®‰æ…°,åªæ˜¯é™é™åœ°å¬ã€‚æˆ‘è¯´äº†å¾ˆå¤š,
ä¹Ÿæ²‰é»˜äº†å¾ˆä¹…ã€‚æœ‰äº›è¯è¯´å‡ºæ¥,å°±è½»äº†ä¸€äº›ã€‚
çª—å¤–çš„é›¨åœäº†,å¿ƒé‡Œçš„é›¨ä¹Ÿæ…¢äº†ä¸‹æ¥ã€‚"

è¯·ç”Ÿæˆæ—¥è®°ã€‚
        """
        
        response = await openai.ChatCompletion.acreate(
            model="gpt-4-turbo-preview",
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯æ¸©æŸ”çš„æ–‡å­—æ²»ç–—å¸ˆ,ç”¨è¯—æ„è®°å½•æƒ…ç»ªã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.8,
            max_tokens=300
        )
        
        diary_text = response.choices[0].message.content
        
        return {
            "date": datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥"),
            "text": diary_text,
            "emotion": emotion_data.get("primary", "unknown"),
            "tags": self._extract_emotion_tags(diary_text)
        }
    
    async def _generate_voice(
        self,
        text: str,
        voice: str = "shimmer",
        speed: float = 0.85
    ) -> Dict:
        """ç”Ÿæˆè¯­éŸ³"""
        
        # è°ƒç”¨OpenAI TTS
        try:
            response = await openai.Audio.acreate(
                model="tts-1-hd",  # é«˜æ¸…ç‰ˆ,éŸ³è´¨æ›´å¥½
                input=text,
                voice=voice,
                speed=speed
            )
            
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            audio_filename = f"healing_{int(datetime.now().timestamp())}.mp3"
            # ... ä¿å­˜é€»è¾‘
            
            return {
                "url": f"/storage/audio/generated/{audio_filename}",
                "duration": len(text) * 0.1 / speed
            }
        except Exception as e:
            print(f"Voice generation error: {e}")
            return {"url": "", "duration": 0}
    
    async def _predict_emotion_shift(
        self,
        user_message: str,
        ai_response: str
    ) -> Dict:
        """é¢„æµ‹æƒ…ç»ªå˜åŒ–è¶‹åŠ¿"""
        
        prompt = f"""
åˆ†æè¿™æ®µå¯¹è¯åç”¨æˆ·çš„æƒ…ç»ªå˜åŒ–:

ç”¨æˆ·: {user_message}
AI: {ai_response}

é¢„æµ‹:
1. æƒ…ç»ªæ˜¯å¦æœ‰æ‰€ç¼“è§£? (0-1)
2. ç”¨æˆ·å¯èƒ½çš„ä¸‹ä¸€æ­¥ååº”
3. å»ºè®®çš„åç»­ç­–ç•¥

ä»¥JSONè¿”å›ã€‚
        """
        
        response = await openai.ChatCompletion.acreate(
            model="gpt-4-turbo-preview",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.5
        )
        
        return json.loads(response.choices[0].message.content)
    
    def _suggest_next_action(self, intensity: float) -> str:
        """æ ¹æ®æƒ…ç»ªå¼ºåº¦å»ºè®®ä¸‹ä¸€æ­¥è¡ŒåŠ¨"""
        
        if intensity > 0.7:
            return "å»ºè®®è¿›è¡Œæ·±åº¦å†¥æƒ³å¼•å¯¼"
        elif intensity > 0.4:
            return "å»ºè®®æ’­æ”¾ç–—æ„ˆéŸ³ä¹"
        else:
            return "å»ºè®®è®°å½•æƒ…ç»ªæ—¥è®°"
    
    def _suggest_activity(self, intensity: float) -> str:
        """å»ºè®®é…åˆçš„æ´»åŠ¨"""
        
        activities = {
            0.8: "æ·±å‘¼å¸,ä¸“æ³¨å½“ä¸‹",
            0.6: "è½»æŸ”çš„ä¼¸å±•è¿åŠ¨",
            0.4: "æ³¡ä¸€æ¯çƒ­èŒ¶,æ…¢æ…¢å“å‘³",
            0.2: "çœ‹çª—å¤–çš„é£æ™¯"
        }
        
        for threshold, activity in sorted(activities.items(), reverse=True):
            if intensity >= threshold:
                return activity
        
        return "è‡ªç”±æ”¾æ¾"
    
    def _extract_emotion_tags(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬æå–æƒ…ç»ªæ ‡ç­¾"""
        
        emotion_keywords = {
            "å­¤ç‹¬": ["å­¤ç‹¬", "ä¸€ä¸ªäºº", "å¯‚å¯"],
            "å¤±è½": ["å¤±è½", "å¤±å»", "é—æ†¾"],
            "ç–²æƒ«": ["ç´¯", "ç–²æƒ«", "è€—å°½"],
            "è¿·èŒ«": ["è¿·èŒ«", "ä¸çŸ¥", "å›°æƒ‘"],
            "å¹³é™": ["å¹³é™", "å®‰é™", "å®é™"],
            "å¸Œæœ›": ["å¸Œæœ›", "æœŸå¾…", "æ˜å¤©"]
        }
        
        tags = []
        for tag, keywords in emotion_keywords.items():
            if any(keyword in text for keyword in keywords):
                tags.append(tag)
        
        return tags[:3]  # æœ€å¤š3ä¸ªæ ‡ç­¾


# ==========================================
# backend-nodejs/src/services/healingService.js
# Node.jsä¾§çš„ç–—æ„ˆæœåŠ¡è°ƒç”¨å°è£…
# ==========================================

"""
// æ–‡ä»¶: backend-nodejs/src/services/healingService.js

const axios = require('axios');

class HealingService {
  constructor(aiServiceUrl) {
    this.aiServiceUrl = aiServiceUrl || 'http://localhost:8000';
  }

  async startHealingConversation(userId, userMessage, emotionIntensity, history = []) {
    try {
      const response = await axios.post(`${this.aiServiceUrl}/api/v1/healing/conversation`, {
        user_id: userId,
        message: userMessage,
        emotion_intensity: emotionIntensity,
        history
      });
      
      return response.data;
    } catch (error) {
      console.error('Healing conversation error:', error);
      throw error;
    }
  }

  async generateHealingMusic(emotionIntensity, duration = 180, style = 'gentle') {
    try {
      const response = await axios.post(`${this.aiServiceUrl}/api/v1/healing/music`, {
        emotion_intensity: emotionIntensity,
        duration,
        style
      });
      
      return response.data;
    } catch (error) {
      console.error('Healing music generation error:', error);
      throw error;
    }
  }

  async generateMeditationGuide(duration = 600, focus = 'breath') {
    try {
      const response = await axios.post(`${this.aiServiceUrl}/api/v1/healing/meditation`, {
        duration,
        focus
      });
      
      return response.data;
    } catch (error) {
      console.error('Meditation guide generation error:', error);
      throw error;
    }
  }

  async createEmotionDiary(userId, conversationSummary, emotionData) {
    try {
      const response = await axios.post(`${this.aiServiceUrl}/api/v1/healing/diary`, {
        user_id: userId,
        conversation_summary: conversationSummary,
        emotion_data: emotionData
      });
      
      // ä¿å­˜åˆ°æ•°æ®åº“
      // await this.saveToDatabase(userId, response.data);
      
      return response.data;
    } catch (error) {
      console.error('Emotion diary creation error:', error);
      throw error;
    }
  }
}

module.exports = HealingService;
"""
</file>

<file path="leiShiAiAgent/backend-ai/app/services/music_composer.py">
"""
AIéŸ³ä¹åˆ›ä½œæœåŠ¡
æ–‡ä»¶: backend-ai/app/services/music_composer.py
åŠŸèƒ½: ä¸ºå¿«ä¹æƒ…ç»ªåˆ›ä½œéŸ³ä¹(å“¼å”±è½¬æ­Œæ›²ã€è‡ªåŠ¨ç¼–æ›²)
"""

import openai
import json
import os
from typing import Dict, List
from datetime import datetime

class MusicComposer:
    def __init__(self):
        self.api_key = os.getenv("OPENAI_API_KEY")
        openai.api_key = self.api_key
        
        self.music_styles = {
            "pop": {"bpm": 120, "structure": "ABABCB", "mood": "æ¬¢å¿«æµè¡Œ"},
            "electronic": {"bpm": 128, "structure": "Intro-Drop-Break-Drop", "mood": "ç”µå­å¾‹åŠ¨"},
            "acoustic": {"bpm": 90, "structure": "AABA", "mood": "æ°‘è°£æ¸…æ–°"},
            "rock": {"bpm": 140, "structure": "ABABCB", "mood": "æ‘‡æ»šæ¿€æƒ…"},
            "jazz": {"bpm": 110, "structure": "AABA", "mood": "çˆµå£«å³å…´"}
        }
    
    async def hum_to_song(
        self,
        audio_data: str,  # base64ç¼–ç çš„å“¼å”±éŸ³é¢‘
        style: str = "pop",
        lyrics_theme: str = None
    ) -> Dict:
        """
        å“¼å”±è½¬å®Œæ•´æ­Œæ›²
        1. è¯†åˆ«å“¼å”±çš„æ—‹å¾‹
        2. ç”Ÿæˆå®Œæ•´ç¼–æ›²
        3. åˆ›ä½œæ­Œè¯
        4. åˆæˆäººå£°
        """
        
        # Step 1: ä½¿ç”¨Whisperè½¬å½•å“¼å”±
        melody_info = await self._analyze_hum(audio_data)
        
        # Step 2: ç”Ÿæˆæ­Œæ›²ç»“æ„
        song_structure = await self._create_song_structure(
            melody_info,
            style,
            lyrics_theme
        )
        
        # Step 3: åˆ›ä½œæ­Œè¯
        lyrics = await self._generate_lyrics(
            song_structure,
            lyrics_theme
        )
        
        # Step 4: ç”Ÿæˆç¼–æ›²æ–¹æ¡ˆ
        arrangement = await self._create_arrangement(
            melody_info,
            song_structure,
            style
        )
        
        return {
            "id": f"song_{int(datetime.now().timestamp())}",
            "title": song_structure["title"],
            "style": style,
            "structure": song_structure,
            "lyrics": lyrics,
            "arrangement": arrangement,
            "audio_url": f"/storage/audio/songs/song_{int(datetime.now().timestamp())}.mp3",
            "stems": {  # åˆ†è½¨
                "melody": "/storage/audio/stems/melody.mp3",
                "drums": "/storage/audio/stems/drums.mp3",
                "bass": "/storage/audio/stems/bass.mp3",
                "harmony": "/storage/audio/stems/harmony.mp3"
            },
            "created_at": datetime.now().isoformat()
        }
    
    async def auto_compose(
        self,
        mood: str,
        style: str,
        duration: int = 180
    ) -> Dict:
        """
        è‡ªåŠ¨ä½œæ›²
        æ ¹æ®æƒ…ç»ªå’Œé£æ ¼è‡ªåŠ¨åˆ›ä½œå®Œæ•´æ­Œæ›²
        """
        
        prompt = f"""
åˆ›ä½œä¸€é¦–{style}é£æ ¼çš„æ­Œæ›²ã€‚

æƒ…ç»ª: {mood}
æ—¶é•¿: {duration}ç§’
é£æ ¼å‚æ•°: {json.dumps(self.music_styles.get(style, {}), ensure_ascii=False)}

è¯·è®¾è®¡å®Œæ•´çš„éŸ³ä¹ç»“æ„:

1. æ­Œæ›²ä¿¡æ¯:
   - æ ‡é¢˜(æœ‰åˆ›æ„çš„)
   - BPM
   - è°ƒæ€§
   - æ•´ä½“æƒ…ç»ªæ›²çº¿

2. æ®µè½ç»“æ„:
   {{
     "intro": {{
       "duration": 8,
       "description": "å‰å¥è®¾è®¡",
       "instruments": ["ä¹å™¨åˆ—è¡¨"],
       "melody": "æ—‹å¾‹æè¿°"
     }},
     "verse": {{
       "duration": 16,
       "description": "ä¸»æ­Œè®¾è®¡",
       "chord_progression": "å’Œå¼¦è¿›è¡Œ",
       "melody_range": "éŸ³åŸŸèŒƒå›´"
     }},
     "chorus": {{
       "duration": 16,
       "description": "å‰¯æ­Œè®¾è®¡",
       "hook": "è®°å¿†ç‚¹è®¾è®¡",
       "energy_level": "èƒ½é‡ç­‰çº§(1-10)"
     }},
     ... å…¶ä»–æ®µè½
   }}

3. é…å™¨æ–¹æ¡ˆ:
   - ä¸»æ—‹å¾‹ä¹å™¨
   - å’Œå£°ä¹å™¨
   - èŠ‚å¥ç»„
   - éŸ³æ•ˆç‚¹ç¼€

4. æ··éŸ³å»ºè®®:
   - å„è½¨éŸ³é‡å¹³è¡¡
   - å£°åƒåˆ†å¸ƒ
   - æ•ˆæœå™¨ä½¿ç”¨

ä»¥JSONæ ¼å¼è¿”å›å®Œæ•´çš„åˆ›ä½œæ–¹æ¡ˆã€‚
        """
        
        response = await openai.ChatCompletion.acreate(
            model="gpt-4-turbo-preview",
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸“ä¸šçš„éŸ³ä¹åˆ¶ä½œäººå’Œä½œæ›²å®¶ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            response_format={"type": "json_object"},
            temperature=0.7
        )
        
        composition = json.loads(response.choices[0].message.content)
        
        return {
            "id": f"composition_{int(datetime.now().timestamp())}",
            "title": composition.get("title", "æœªå‘½åä½œå“"),
            "style": style,
            "mood": mood,
            "composition": composition,
            "audio_url": f"/storage/audio/compositions/{int(datetime.now().timestamp())}.mp3",
            "sheet_music_url": f"/storage/sheets/{int(datetime.now().timestamp())}.pdf"
        }
    
    async def remix_song(
        self,
        original_song_id: str,
        remix_style: str,
        user_preferences: Dict
    ) -> Dict:
        """
        æ­Œæ›²æ··éŸ³
        å¯¹å·²æœ‰æ­Œæ›²è¿›è¡Œä¸ªæ€§åŒ–æ··éŸ³
        """
        
        # è·å–åŸå§‹æ­Œæ›²ä¿¡æ¯
        # original = await get_song(original_song_id)
        
        prompt = f"""
ä¸ºä¸€é¦–æ­Œæ›²è®¾è®¡æ··éŸ³æ–¹æ¡ˆã€‚

æ··éŸ³é£æ ¼: {remix_style}
ç”¨æˆ·åå¥½: {json.dumps(user_preferences, ensure_ascii=False)}

è¯·è®¾è®¡:
1. æ•´ä½“æ··éŸ³é£æ ¼
2. æ¯ä¸ªéŸ³è½¨çš„å¤„ç†:
   - EQè®¾ç½®
   - å‹ç¼©å‚æ•°
   - æ•ˆæœå™¨é“¾
   - éŸ³é‡è‡ªåŠ¨åŒ–
3. ç©ºé—´æ„Ÿè®¾è®¡:
   - Reverbè®¾ç½®
   - Delayè¿ç”¨
   - å£°åƒåˆ†å¸ƒ
4. åˆ›æ„ç‚¹:
   - ç‰¹æ®ŠéŸ³æ•ˆ
   - è¿‡æ¸¡æŠ€å·§
   - æƒŠå–œå…ƒç´ 

ä»¥JSONè¿”å›æ··éŸ³æ–¹æ¡ˆã€‚
        """
        
        response = await openai.ChatCompletion.acreate(
            model="gpt-4-turbo-preview",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.7
        )
        
        remix_plan = json.loads(response.choices[0].message.content)
        
        return {
            "id": f"remix_{original_song_id}_{int(datetime.now().timestamp())}",
            "original_song_id": original_song_id,
            "remix_style": remix_style,
            "remix_plan": remix_plan,
            "audio_url": f"/storage/audio/remixes/{int(datetime.now().timestamp())}.mp3"
        }
    
    async def _analyze_hum(self, audio_data: str) -> Dict:
        """åˆ†æå“¼å”±éŸ³é¢‘"""
        
        # ä½¿ç”¨Whisperè½¬å½•(è™½ç„¶æ˜¯å“¼å”±,ä½†å¯ä»¥æå–éŸ³è°ƒä¿¡æ¯)
        # å®é™…åº”ç”¨ä¸­éœ€è¦éŸ³ä¹åˆ†æåº“(å¦‚librosa)
        
        return {
            "key": "C Major",
            "bpm": 120,
            "melody_contour": [60, 62, 64, 65, 67],  # MIDIéŸ³ç¬¦
            "rhythm_pattern": [1, 0.5, 0.5, 1, 1],
            "duration": 8
        }
    
    async def _create_song_structure(
        self,
        melody_info: Dict,
        style: str,
        theme: str
    ) -> Dict:
        """åˆ›å»ºæ­Œæ›²ç»“æ„"""
        
        style_template = self.music_styles.get(style, self.music_styles["pop"])
        
        prompt = f"""
åŸºäºå“¼å”±çš„æ—‹å¾‹,è®¾è®¡å®Œæ•´æ­Œæ›²ç»“æ„ã€‚

æ—‹å¾‹ä¿¡æ¯: {json.dumps(melody_info, ensure_ascii=False)}
é£æ ¼: {style}
ä¸»é¢˜: {theme or 'è‡ªç”±å‘æŒ¥'}

è¯·è®¾è®¡:
1. æ­Œæ›²æ ‡é¢˜(è¦æœ‰åˆ›æ„å’Œæ„å¢ƒ)
2. å®Œæ•´ç»“æ„(Intro-Verse-Chorus-Bridge-Outro)
3. æ¯æ®µçš„æƒ…ç»ªå’Œèƒ½é‡
4. æ—‹å¾‹å‘å±•ç­–ç•¥
5. å’Œå£°è¿›è¡Œ

ä»¥JSONè¿”å›ã€‚
        """
        
        response = await openai.ChatCompletion.acreate(
            model="gpt-4-turbo-preview",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        
        return json.loads(response.choices[0].message.content)
    
    async def _generate_lyrics(
        self,
        song_structure: Dict,
        theme: str
    ) -> Dict:
        """ç”Ÿæˆæ­Œè¯"""
        
        prompt = f"""
ä¸ºæ­Œæ›²åˆ›ä½œæ­Œè¯ã€‚

æ­Œæ›²æ ‡é¢˜: {song_structure.get('title', 'æœªå‘½å')}
ä¸»é¢˜: {theme or 'å¿«ä¹ã€è‡ªç”±'}
ç»“æ„: {json.dumps(song_structure, ensure_ascii=False)}

è¦æ±‚:
1. ç¬¦åˆæ­Œæ›²æƒ…ç»ªå’Œèƒ½é‡æ›²çº¿
2. æœ‰ç”»é¢æ„Ÿå’Œæ„å¢ƒ
3. æŠ¼éŸµè‡ªç„¶
4. å‰¯æ­Œè¦æœ‰è®°å¿†ç‚¹
5. é€‚åˆæ¼”å”±

ä»¥JSONè¿”å›:
{{
  "verse1": ["ç¬¬ä¸€å¥", "ç¬¬äºŒå¥", ...],
  "chorus": ["å‰¯æ­Œç¬¬ä¸€å¥", ...],
  "verse2": [...],
  "bridge": [...],
  "chorus_repeat": [...]
}}
        """
        
        response = await openai.ChatCompletion.acreate(
            model="gpt-4-turbo-preview",
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¼˜ç§€çš„ä½œè¯äºº,æ“…é•¿åˆ›ä½œæœ‰æ„å¢ƒçš„æ­Œè¯ã€‚"
                },
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.8
        )
        
        return json.loads(response.choices[0].message.content)
    
    async def _create_arrangement(
        self,
        melody_info: Dict,
        song_structure: Dict,
        style: str
    ) -> Dict:
        """åˆ›å»ºç¼–æ›²æ–¹æ¡ˆ"""
        
        prompt = f"""
ä¸ºæ­Œæ›²è®¾è®¡å®Œæ•´ç¼–æ›²ã€‚

é£æ ¼: {style}
æ—‹å¾‹: {json.dumps(melody_info, ensure_ascii=False)}
ç»“æ„: {json.dumps(song_structure, ensure_ascii=False)}

è¯·è®¾è®¡:
1. ä¹å™¨é…ç½®
2. æ¯æ®µçš„ç¼–æ›²å±‚æ¬¡
3. ç»‡ä½“å˜åŒ–
4. éŸ³è‰²é€‰æ‹©
5. æ•ˆæœä½¿ç”¨

ä»¥JSONè¿”å›è¯¦ç»†ç¼–æ›²æ–¹æ¡ˆã€‚
        """
        
        response = await openai.ChatCompletion.acreate(
            model="gpt-4-turbo-preview",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        
        return json.loads(response.choices[0].message.content)


# ==========================================
# è®°å¿†ç®¡ç†æœåŠ¡
# backend-ai/app/services/memory_manager.py
# ==========================================

class MemoryManager:
    """
    é•¿æœŸè®°å¿†ç®¡ç†
    åŠŸèƒ½: å­˜å‚¨ã€æ£€ç´¢ã€æ€»ç»“ç”¨æˆ·çš„æƒ…æ„Ÿå†ç¨‹
    """
    
    def __init__(self):
        self.api_key = os.getenv("OPENAI_API_KEY")
        openai.api_key = self.api_key
    
    async def save_memory(
        self,
        user_id: str,
        memory_type: str,  # 'conversation', 'music', 'podcast', 'diary'
        content: Dict,
        emotion: str
    ) -> Dict:
        """ä¿å­˜è®°å¿†"""
        
        # ç”Ÿæˆè®°å¿†æ‘˜è¦
        summary = await self._generate_summary(memory_type, content)
        
        # æå–å…³é”®æ ‡ç­¾
        tags = await self._extract_tags(content)
        
        memory = {
            "id": f"memory_{user_id}_{int(datetime.now().timestamp())}",
            "user_id": user_id,
            "type": memory_type,
            "summary": summary,
            "content": content,
            "emotion": emotion,
            "tags": tags,
            "created_at": datetime.now().isoformat()
        }
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        # await db.memories.insert(memory)
        
        return memory
    
    async def retrieve_memories(
        self,
        user_id: str,
        filters: Dict = None,
        limit: int = 20
    ) -> List[Dict]:
        """æ£€ç´¢è®°å¿†"""
        
        # ä»æ•°æ®åº“æŸ¥è¯¢
        # memories = await db.memories.find(user_id, filters, limit)
        
        # æŒ‰æ—¶é—´æ’åº
        # memories.sort(key=lambda x: x['created_at'], reverse=True)
        
        return []  # è¿”å›è®°å¿†åˆ—è¡¨
    
    async def generate_memory_summary(
        self,
        user_id: str,
        time_range: str = "week"
    ) -> Dict:
        """
        ç”Ÿæˆè®°å¿†æ€»ç»“
        æ€»ç»“ç”¨æˆ·ä¸€æ®µæ—¶é—´çš„æƒ…æ„Ÿå†ç¨‹
        """
        
        # è·å–æ—¶é—´èŒƒå›´å†…çš„è®°å¿†
        # memories = await self.retrieve_memories(user_id, {"time_range": time_range})
        
        prompt = f"""
æ€»ç»“ç”¨æˆ·è¿™æ®µæ—¶é—´çš„æƒ…æ„Ÿå†ç¨‹ã€‚

æ—¶é—´èŒƒå›´: {time_range}
è®°å¿†æ•°é‡: Xæ¡

è¯·ç”Ÿæˆ:
1. æ•´ä½“æƒ…æ„Ÿè¶‹åŠ¿
2. ä¸»è¦æƒ…ç»ªå˜åŒ–
3. é‡è¦æ—¶åˆ»
4. æˆé•¿å’Œå˜åŒ–
5. æ¸©æš–çš„é¼“åŠ±

ç”¨æ¸©æŸ”ã€è¯—æ„çš„è¯­è¨€,åƒæœ‹å‹èˆ¬ä¹¦å†™ã€‚
å­—æ•°: 200-300å­—

ä»¥JSONè¿”å›:
{{
  "title": "è¿™å‘¨çš„å£°éŸ³æ—…ç¨‹",
  "summary": "æ€»ç»“æ–‡æœ¬",
  "emotion_trend": "æƒ…æ„Ÿæ›²çº¿æè¿°",
  "highlights": ["äº®ç‚¹1", "äº®ç‚¹2"],
  "encouragement": "é¼“åŠ±çš„è¯"
}}
        """
        
        response = await openai.ChatCompletion.acreate(
            model="gpt-4-turbo-preview",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.7
        )
        
        return json.loads(response.choices[0].message.content)
    
    async def create_memory_collage(
        self,
        user_id: str,
        memory_ids: List[str]
    ) -> Dict:
        """
        åˆ›å»ºè®°å¿†æ‹¼è´´
        å°†å¤šä¸ªè®°å¿†ç»„åˆæˆä¸€ä¸ªä½œå“
        """
        
        # è·å–è®°å¿†å†…å®¹
        # memories = [await get_memory(mid) for mid in memory_ids]
        
        prompt = f"""
å°†è¿™äº›è®°å¿†ç‰‡æ®µç»„åˆæˆä¸€ä¸ªå®Œæ•´çš„æ•…äº‹æˆ–ä½œå“ã€‚

è®°å¿†ç±»å‹: [éŸ³ä¹ã€å¯¹è¯ã€æ’­å®¢ã€æ—¥è®°]

è¯·åˆ›ä½œ:
1. ä¸€ä¸ªæ ‡é¢˜
2. å°†è®°å¿†ä¸²è”æˆè¯—æ„çš„å™äº‹
3. é…åˆåˆé€‚çš„èƒŒæ™¯éŸ³ä¹å»ºè®®
4. æƒ…æ„Ÿçº¿ç´¢

ç”¨ç¬¬ä¸€äººç§°"æˆ‘"ä¹¦å†™,åƒå›å¿†å½•èˆ¬æ¸©æš–ã€‚

ä»¥JSONè¿”å›åˆ›ä½œç»“æœã€‚
        """
        
        response = await openai.ChatCompletion.acreate(
            model="gpt-4-turbo-preview",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.8
        )
        
        collage = json.loads(response.choices[0].message.content)
        
        return {
            "id": f"collage_{user_id}_{int(datetime.now().timestamp())}",
            "title": collage["title"],
            "narrative": collage["narrative"],
            "music_suggestion": collage["music"],
            "memory_ids": memory_ids,
            "created_at": datetime.now().isoformat()
        }
    
    async def _generate_summary(
        self,
        memory_type: str,
        content: Dict
    ) -> str:
        """ç”Ÿæˆè®°å¿†æ‘˜è¦"""
        
        prompt = f"ç”¨ä¸€å¥è¯(20å­—å†…)æ¦‚æ‹¬è¿™æ®µ{memory_type}è®°å¿†: {str(content)[:200]}"
        
        response = await openai.ChatCompletion.acreate(
            model="gpt-4-turbo-preview",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=50
        )
        
        return response.choices[0].message.content
    
    async def _extract_tags(self, content: Dict) -> List[str]:
        """æå–æ ‡ç­¾"""
        
        prompt = f"ä»å†…å®¹ä¸­æå–3-5ä¸ªå…³é”®æ ‡ç­¾: {str(content)[:300]}"
        
        response = await openai.ChatCompletion.acreate(
            model="gpt-4-turbo-preview",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5
        )
        
        tags_text = response.choices[0].message.content
        return [tag.strip() for tag in tags_text.split(',')][:5]
</file>

<file path="leiShiAiAgent/backend-ai/app/services/podcast_generator.py">
"""
å£°éŸ³å‰§åœº - æ’­å®¢/ç”µå°ç”ŸæˆæœåŠ¡
æ–‡ä»¶: backend-ai/app/services/podcast_generator.py
åŠŸèƒ½: ä¸ºå¹³é™æƒ…ç»ªç”Ÿæˆæ’­å®¢ã€ç”µå°ã€æœ‰å£°ä¹¦å†…å®¹
"""

import openai
import json
import os
from typing import Dict, List
from datetime import datetime

class PodcastGenerator:
    def __init__(self):
        self.api_key = os.getenv("OPENAI_API_KEY")
        openai.api_key = self.api_key
        
        # å†…å®¹ç±»å‹æ¨¡æ¿
        self.content_types = {
            "history": {
                "name": "å†å²æ—¶åˆ»",
                "description": "é‡æ¸©å†å²,æ„Ÿæ‚Ÿäººç”Ÿ",
                "topics": ["å¤ä»£æ™ºæ…§", "å†å²äººç‰©", "æ–‡æ˜æ¼”å˜", "æˆ˜äº‰ä¸å’Œå¹³"]
            },
            "culture": {
                "name": "äººæ–‡æ¼«è°ˆ",
                "description": "è‰ºæœ¯ã€æ–‡å­¦ã€å“²å­¦çš„è¯—æ„å¯¹è¯",
                "topics": ["è¯—æ­Œèµæ", "ç”µå½±è‰ºæœ¯", "å“²å­¦æ€è€ƒ", "æ–‡åŒ–ç¬¦å·"]
            },
            "science": {
                "name": "ç§‘å­¦ä¹‹å£°",
                "description": "ç”¨è¯—æ„è§£è¯»ç§‘å­¦ä¹‹ç¾",
                "topics": ["å®‡å®™å¥¥ç§˜", "ç”Ÿå‘½èµ·æº", "ç‰©ç†ä¹‹ç¾", "æ•°å­¦ä¹‹ç¾"]
            },
            "story": {
                "name": "æ·±å¤œæ•…äº‹",
                "description": "æ¸©æš–æ²»æ„ˆçš„ç¡å‰æ•…äº‹",
                "topics": ["äººç”Ÿæ•…äº‹", "åŸå¸‚ä¼ è¯´", "æ¸©æƒ…ç¬é—´", "å“²ç†å¯“è¨€"]
            },
            "music": {
                "name": "éŸ³ä¹ç”µå°",
                "description": "éŸ³ä¹èƒŒåçš„æ•…äº‹",
                "topics": ["ç»å…¸æ­Œæ›²", "éŸ³ä¹å®¶", "éŸ³ä¹æµæ´¾", "åˆ›ä½œæ•…äº‹"]
            }
        }
    
    async def generate_podcast_episode(
        self,
        content_type: str,
        duration: int = 600,  # 10åˆ†é’Ÿ
        user_interests: List[str] = None
    ) -> Dict:
        """
        ç”Ÿæˆæ’­å®¢èŠ‚ç›®
        """
        
        content_template = self.content_types.get(
            content_type,
            self.content_types["story"]
        )
        
        # é€‰æ‹©è¯é¢˜
        topic = await self._select_topic(content_type, user_interests)
        
        # ç”ŸæˆèŠ‚ç›®è„šæœ¬
        script = await self._generate_script(
            content_type,
            topic,
            duration
        )
        
        # ç”ŸæˆéŸ³é¢‘(å¤šå£°éƒ¨)
        audio_segments = await self._generate_audio_segments(script)
        
        return {
            "id": f"podcast_{content_type}_{int(datetime.now().timestamp())}",
            "title": script["title"],
            "description": script["description"],
            "content_type": content_template["name"],
            "duration": duration,
            "script": script,
            "audio_segments": audio_segments,
            "full_audio_url": f"/storage/audio/podcast/{content_type}_{int(datetime.now().timestamp())}.mp3",
            "transcript": script["full_text"],
            "tags": script["tags"],
            "created_at": datetime.now().isoformat()
        }
    
    async def generate_radio_show(
        self,
        theme: str,
        duration: int = 1800  # 30åˆ†é’Ÿ
    ) -> Dict:
        """
        ç”Ÿæˆç”µå°èŠ‚ç›®
        """
        
        prompt = f"""
åˆ›ä½œä¸€æœŸ{duration//60}åˆ†é’Ÿçš„æ·±å¤œç”µå°èŠ‚ç›®,ä¸»é¢˜: {theme}

èŠ‚ç›®ç»“æ„:
1. å¼€åœºç™½(1-2åˆ†é’Ÿ): æ¸©æš–çš„é—®å€™,å¼•å…¥ä¸»é¢˜
2. ä¸»è¦å†…å®¹(20-25åˆ†é’Ÿ):
   - 3-4ä¸ªç›¸å…³è¯é¢˜æˆ–æ•…äº‹
   - æ¯ä¸ªè¯é¢˜ä¹‹é—´æœ‰éŸ³ä¹è¿‡æ¸¡
3. äº’åŠ¨ç¯èŠ‚(3-5åˆ†é’Ÿ): è¯»å¬ä¼—æ¥ä¿¡(è™šæ„ä½†çœŸå®)
4. ç»“æŸè¯­(2åˆ†é’Ÿ): æ¸©æš–é“åˆ«,ç•™ä¸‹æ€è€ƒ

è¯­è¨€é£æ ¼:
- ç¬¬ä¸€äººç§°"æˆ‘"å’Œç¬¬äºŒäººç§°"ä½ "äº¤æ›¿
- åƒæ·±å¤œçŸ¥å·±èˆ¬å€¾è¯‰
- é€‚å½“ç•™ç™½å’Œåœé¡¿
- ç”¨"(éŸ³ä¹)"æ ‡æ³¨éŸ³ä¹æ’å…¥ç‚¹
- è¯­é€Ÿæ ‡è®°: (æ…¢é€Ÿ)(å¸¸é€Ÿ)(åœé¡¿)

éŸ³ä¹é€‰æ‹©:
- å¼€åœº: æ¸©æš–çš„çˆµå£«ä¹
- è¿‡æ¸¡: è½»æŸ”çš„é’¢ç´æ›²
- ç»“å°¾: æ²»æ„ˆç³»å‰ä»–

ç¤ºä¾‹ç‰‡æ®µ:
"åˆå¤œå¥½,æˆ‘æ˜¯ä½ çš„å£°éŸ³æœ‹å‹...(åœé¡¿)
çª—å¤–æ˜¯ä¸æ˜¯åˆä¸‹é›¨äº†?è¿™æ ·çš„å¤œæ™š,
é€‚åˆèŠèŠé‚£äº›è—åœ¨å¿ƒé‡Œçš„æ•…äº‹...(éŸ³ä¹æ·¡å…¥)"

è¯·ç”Ÿæˆå®Œæ•´çš„èŠ‚ç›®è„šæœ¬,ä»¥JSONæ ¼å¼è¿”å›:
{{
  "title": "èŠ‚ç›®æ ‡é¢˜",
  "opening": "å¼€åœºç™½",
  "segments": [
    {{
      "type": "content/music/interaction",
      "text": "å†…å®¹",
      "duration": æ—¶é•¿(ç§’)
    }}
  ],
  "closing": "ç»“æŸè¯­",
  "music_list": ["éŸ³ä¹1", "éŸ³ä¹2"],
  "emotional_arc": "æƒ…ç»ªæ›²çº¿æè¿°"
}}
        """
        
        response = await openai.ChatCompletion.acreate(
            model="gpt-4-turbo-preview",
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯èµ„æ·±çš„æ·±å¤œç”µå°ä¸»æŒäºº,å£°éŸ³æ¸©æš–è€Œå¯Œæœ‰ç£æ€§ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            response_format={"type": "json_object"},
            temperature=0.8
        )
        
        script = json.loads(response.choices[0].message.content)
        
        # ç”Ÿæˆä¸»æŒäººéŸ³é¢‘
        audio_segments = await self._generate_radio_audio(script)
        
        return {
            "id": f"radio_{theme}_{int(datetime.now().timestamp())}",
            "title": script["title"],
            "theme": theme,
            "duration": duration,
            "script": script,
            "audio_url": f"/storage/audio/radio/{theme}_{int(datetime.now().timestamp())}.mp3",
            "segments": audio_segments,
            "music_list": script.get("music_list", [])
        }
    
    async def generate_audiobook_chapter(
        self,
        book_genre: str,
        chapter_number: int,
        previous_summary: str = None
    ) -> Dict:
        """
        ç”Ÿæˆæœ‰å£°ä¹¦ç« èŠ‚
        """
        
        genres = {
            "philosophy": "å“²å­¦éšç¬”",
            "fiction": "æ–‡å­¦å°è¯´",
            "biography": "äººç‰©ä¼ è®°",
            "essay": "æ•£æ–‡è¯—é›†"
        }
        
        prompt = f"""
åˆ›ä½œä¸€ä¸ª{genres.get(book_genre, 'æ•…äº‹')}çš„ç¬¬{chapter_number}ç« ã€‚

{f'å‰æƒ…æè¦: {previous_summary}' if previous_summary else 'è¿™æ˜¯ç¬¬ä¸€ç« '}

è¦æ±‚:
1. ç« èŠ‚é•¿åº¦: 2000-3000å­—
2. è¯­è¨€: ä¼˜ç¾ã€æœ‰ç”»é¢æ„Ÿã€é€‚åˆæœ—è¯»
3. èŠ‚å¥: ç¼“æ€¥æœ‰è‡´,é€‚åˆå€¾å¬
4. ç»“æ„: 
   - å¼€å¤´: å¼•äººå…¥èƒœ
   - ä¸­æ®µ: æƒ…èŠ‚/æ€æƒ³å±•å¼€
   - ç»“å°¾: ç•™æœ‰ä½™éŸµ
5. ç”¨"(åœé¡¿)"æ ‡æ³¨è‡ªç„¶åœé¡¿ç‚¹
6. é€‚åˆæˆäººæ·±åº¦é˜…è¯»

é£æ ¼å‚è€ƒ: ä½™åã€è«è¨€ã€ä¸‰æ¯›çš„å™äº‹é£æ ¼

ç”Ÿæˆç« èŠ‚å†…å®¹å’Œæ‘˜è¦ã€‚
        """
        
        response = await openai.ChatCompletion.acreate(
            model="gpt-4-turbo-preview",
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¼˜ç§€çš„æ–‡å­¦åˆ›ä½œè€…å’Œæœ—è¯»è€…ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.8,
            max_tokens=4000
        )
        
        chapter_content = response.choices[0].message.content
        
        # ç”Ÿæˆæœ—è¯»éŸ³é¢‘
        audio = await self._generate_audiobook_audio(chapter_content)
        
        # ç”Ÿæˆç« èŠ‚æ‘˜è¦
        summary = await self._generate_chapter_summary(chapter_content)
        
        return {
            "chapter_number": chapter_number,
            "genre": book_genre,
            "content": chapter_content,
            "summary": summary,
            "audio_url": audio["url"],
            "duration": audio["duration"],
            "word_count": len(chapter_content)
        }
    
    async def _select_topic(
        self,
        content_type: str,
        user_interests: List[str] = None
    ) -> str:
        """æ™ºèƒ½é€‰æ‹©è¯é¢˜"""
        
        template = self.content_types.get(content_type)
        available_topics = template["topics"]
        
        if user_interests:
            # æ ¹æ®ç”¨æˆ·å…´è¶£æ¨è
            prompt = f"""
            å¯é€‰è¯é¢˜: {available_topics}
            ç”¨æˆ·å…´è¶£: {user_interests}
            
            æ¨èæœ€åˆé€‚çš„è¯é¢˜,å¹¶è¯´æ˜ç†ç”±ã€‚
            """
            
            response = await openai.ChatCompletion.acreate(
                model="gpt-4-turbo-preview",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.5,
                max_tokens=100
            )
            
            return response.choices[0].message.content
        else:
            # éšæœºé€‰æ‹©
            import random
            return random.choice(available_topics)
    
    async def _generate_script(
        self,
        content_type: str,
        topic: str,
        duration: int
    ) -> Dict:
        """ç”Ÿæˆæ’­å®¢è„šæœ¬"""
        
        word_count = duration * 2.5  # å‡è®¾æ¯ç§’2.5ä¸ªå­—
        
        prompt = f"""
åˆ›ä½œä¸€æœŸæ’­å®¢èŠ‚ç›®è„šæœ¬ã€‚

ç±»å‹: {content_type}
è¯é¢˜: {topic}
æ—¶é•¿: {duration//60}åˆ†é’Ÿ
å­—æ•°: çº¦{int(word_count)}å­—

è„šæœ¬ç»“æ„:
1. æ ‡é¢˜: å¸å¼•äººçš„æ ‡é¢˜
2. å¼•è¨€(100å­—): å¼€åœºç™½
3. ä¸»ä½“(æ ¸å¿ƒå†…å®¹):
   - åˆ†3-4ä¸ªå°èŠ‚
   - æ¯èŠ‚æœ‰å°æ ‡é¢˜
   - å†…å®¹æ·±å…¥æµ…å‡º
   - åŒ…å«æ•…äº‹ã€ä¾‹å­ã€æ€è€ƒ
4. ç»“è¯­(100å­—): æ€»ç»“å’Œæ„Ÿæ‚Ÿ

è¯­è¨€è¦æ±‚:
- å£è¯­åŒ–,åƒé¢å¯¹é¢èŠå¤©
- ç”¨"ä½ "å’Œ"æˆ‘"æ‹‰è¿‘è·ç¦»
- é€‚å½“æé—®,å¼•å‘æ€è€ƒ
- é¿å…è¯´æ•™,å¤šç”¨æ•…äº‹
- æ ‡æ³¨åœé¡¿ç‚¹: (åœé¡¿)

ç¤ºä¾‹å¼€å¤´:
"å—¨,ä»Šå¤©æƒ³å’Œä½ èŠèŠ...(åœé¡¿)
ä½ æœ‰æ²¡æœ‰æƒ³è¿‡,ä¸ºä»€ä¹ˆ..."

è¯·ä»¥JSONæ ¼å¼è¿”å›:
{{
  "title": "æ ‡é¢˜",
  "description": "ç®€ä»‹",
  "intro": "å¼•è¨€",
  "sections": [
    {{
      "subtitle": "å°èŠ‚æ ‡é¢˜",
      "content": "å†…å®¹",
      "duration": é¢„ä¼°æ—¶é•¿
    }}
  ],
  "outro": "ç»“è¯­",
  "full_text": "å®Œæ•´æ–‡æœ¬",
  "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"],
  "key_points": ["è¦ç‚¹1", "è¦ç‚¹2"]
}}
        """
        
        response = await openai.ChatCompletion.acreate(
            model="gpt-4-turbo-preview",
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸“ä¸šçš„æ’­å®¢ä¸»æ’­,å–„äºç”¨å£°éŸ³è®²æ•…äº‹ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            response_format={"type": "json_object"},
            temperature=0.7,
            max_tokens=3000
        )
        
        return json.loads(response.choices[0].message.content)
    
    async def _generate_audio_segments(
        self,
        script: Dict
    ) -> List[Dict]:
        """ç”ŸæˆéŸ³é¢‘ç‰‡æ®µ"""
        
        segments = []
        
        # å¼•è¨€
        intro_audio = await self._text_to_speech(
            script["intro"],
            voice="alloy",
            speed=0.95
        )
        segments.append({
            "type": "intro",
            "text": script["intro"],
            "audio_url": intro_audio["url"],
            "duration": intro_audio["duration"]
        })
        
        # å„å°èŠ‚
        for idx, section in enumerate(script["sections"]):
            section_audio = await self._text_to_speech(
                section["content"],
                voice="alloy",
                speed=0.95
            )
            segments.append({
                "type": "section",
                "index": idx + 1,
                "subtitle": section["subtitle"],
                "text": section["content"],
                "audio_url": section_audio["url"],
                "duration": section_audio["duration"]
            })
        
        # ç»“è¯­
        outro_audio = await self._text_to_speech(
            script["outro"],
            voice="alloy",
            speed=0.9
        )
        segments.append({
            "type": "outro",
            "text": script["outro"],
            "audio_url": outro_audio["url"],
            "duration": outro_audio["duration"]
        })
        
        return segments
    
    async def _generate_radio_audio(
        self,
        script: Dict
    ) -> List[Dict]:
        """ç”Ÿæˆç”µå°éŸ³é¢‘"""
        
        audio_segments = []
        
        # å¼€åœº
        opening_audio = await self._text_to_speech(
            script["opening"],
            voice="onyx",  # æ›´æœ‰ç£æ€§çš„å£°éŸ³
            speed=0.9
        )
        audio_segments.append({
            "type": "opening",
            "text": script["opening"],
            "audio_url": opening_audio["url"]
        })
        
        # å„ä¸ªç‰‡æ®µ
        for segment in script["segments"]:
            if segment["type"] == "music":
                audio_segments.append({
                    "type": "music",
                    "name": segment["text"],
                    "duration": segment["duration"]
                })
            else:
                seg_audio = await self._text_to_speech(
                    segment["text"],
                    voice="onyx",
                    speed=0.9
                )
                audio_segments.append({
                    "type": segment["type"],
                    "text": segment["text"],
                    "audio_url": seg_audio["url"]
                })
        
        # ç»“æŸ
        closing_audio = await self._text_to_speech(
            script["closing"],
            voice="onyx",
            speed=0.85
        )
        audio_segments.append({
            "type": "closing",
            "text": script["closing"],
            "audio_url": closing_audio["url"]
        })
        
        return audio_segments
    
    async def _generate_audiobook_audio(
        self,
        content: str
    ) -> Dict:
        """ç”Ÿæˆæœ‰å£°ä¹¦éŸ³é¢‘"""
        
        # åˆ†æ®µå¤„ç†é•¿æ–‡æœ¬
        max_chunk = 4000  # OpenAI TTSé™åˆ¶
        chunks = [content[i:i+max_chunk] for i in range(0, len(content), max_chunk)]
        
        audio_urls = []
        total_duration = 0
        
        for chunk in chunks:
            audio = await self._text_to_speech(
                chunk,
                voice="nova",  # é€‚åˆæœ—è¯»çš„å£°éŸ³
                speed=0.9
            )
            audio_urls.append(audio["url"])
            total_duration += audio["duration"]
        
        # å®é™…åº”ç”¨ä¸­éœ€è¦åˆå¹¶éŸ³é¢‘æ–‡ä»¶
        return {
            "url": f"/storage/audio/audiobook/{int(datetime.now().timestamp())}.mp3",
            "duration": total_duration,
            "segments": audio_urls
        }
    
    async def _text_to_speech(
        self,
        text: str,
        voice: str = "alloy",
        speed: float = 1.0
    ) -> Dict:
        """æ–‡æœ¬è½¬è¯­éŸ³"""
        
        try:
            response = await openai.Audio.acreate(
                model="tts-1-hd",
                input=text,
                voice=voice,
                speed=speed
            )
            
            filename = f"tts_{int(datetime.now().timestamp())}_{hash(text)}.mp3"
            # ä¿å­˜éŸ³é¢‘...
            
            return {
                "url": f"/storage/audio/generated/{filename}",
                "duration": len(text) * 0.12 / speed  # ç²—ç•¥ä¼°ç®—
            }
        except Exception as e:
            print(f"TTS Error: {e}")
            return {"url": "", "duration": 0}
    
    async def _generate_chapter_summary(
        self,
        content: str
    ) -> str:
        """ç”Ÿæˆç« èŠ‚æ‘˜è¦"""
        
        prompt = f"ç”¨50å­—æ¦‚æ‹¬è¿™ä¸€ç« çš„æ ¸å¿ƒå†…å®¹:\n\n{content[:500]}..."
        
        response = await openai.ChatCompletion.acreate(
            model="gpt-4-turbo-preview",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=100
        )
        
        return response.choices[0].message.content
</file>

<file path="leiShiAiAgent/backend-ai/app/main.py">
"""
AI Emotion Companion - FastAPI AI Agent
æ–‡ä»¶: backend/ai-agent/app/main.py
åŠŸèƒ½: AIä»£ç†ä¸»æœåŠ¡,å¤„ç†æƒ…æ„Ÿè¯†åˆ«ã€æ•…äº‹ç”Ÿæˆã€éŸ³ä¹æ··éŸ³
"""

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, File, UploadFile, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import List, Optional, Dict
import asyncio
import json
from datetime import datetime

from app.services.emotion_analyzer import EmotionAnalyzer
from app.services.story_generator import StoryGenerator
from app.services.music_mixer import MusicMixer
from app.services.voice_synthesizer import VoiceSynthesizer

app = FastAPI(
    title="AI Emotion Companion API",
    description="åŸºäºæƒ…æ„Ÿè¯†åˆ«çš„AIäº¤äº’ç³»ç»Ÿ",
    version="1.0.0"
)

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ–æœåŠ¡
emotion_analyzer = EmotionAnalyzer()
story_generator = StoryGenerator()
music_mixer = MusicMixer()
voice_synthesizer = VoiceSynthesizer()

# WebSocketè¿æ¥ç®¡ç†
class ConnectionManager:
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}
    
    async def connect(self, websocket: WebSocket, client_id: str):
        await websocket.accept()
        self.active_connections[client_id] = websocket
    
    def disconnect(self, client_id: str):
        if client_id in self.active_connections:
            del self.active_connections[client_id]
    
    async def send_message(self, message: dict, client_id: str):
        if client_id in self.active_connections:
            await self.active_connections[client_id].send_json(message)
    
    async def broadcast(self, message: dict):
        for connection in self.active_connections.values():
            await connection.send_json(message)

manager = ConnectionManager()

# æ•°æ®æ¨¡å‹
class EmotionRequest(BaseModel):
    audio_data: Optional[str] = None
    text: Optional[str] = None
    scene: str  # "car", "ktv", "story"
    user_age: int
    group_size: int

class SceneConfig(BaseModel):
    scene_type: str
    participants: List[Dict]
    settings: Dict

class MusicMixRequest(BaseModel):
    emotions: List[str]
    participants: List[Dict]
    style: str

# APIç«¯ç‚¹
@app.get("/")
async def root():
    return {
        "service": "AI Emotion Companion",
        "status": "running",
        "timestamp": datetime.now().isoformat()
    }

@app.post("/api/v1/emotion/analyze")
async def analyze_emotion(request: EmotionRequest):
    """åˆ†ææƒ…ç»ª - æ”¯æŒéŸ³é¢‘å’Œæ–‡æœ¬è¾“å…¥"""
    try:
        result = await emotion_analyzer.analyze(
            audio_data=request.audio_data,
            text=request.text,
            scene=request.scene,
            user_age=request.user_age
        )
        
        return JSONResponse(content={
            "success": True,
            "data": {
                "emotion": result["emotion"],
                "confidence": result["confidence"],
                "valence": result["valence"],  # æƒ…æ„Ÿæ­£è´Ÿå‘
                "arousal": result["arousal"],  # æƒ…æ„Ÿæ¿€æ´»åº¦
                "suggestions": result["suggestions"],
                "timestamp": datetime.now().isoformat()
            }
        })
    except Exception as e:
        return JSONResponse(status_code=500, content={
            "success": False,
            "error": str(e)
        })

@app.post("/api/v1/story/generate")
async def generate_story(scene_config: SceneConfig):
    """ç”ŸæˆAIäº’åŠ¨æ•…äº‹"""
    try:
        story = await story_generator.create_story(
            scene_type=scene_config.scene_type,
            participants=scene_config.participants,
            settings=scene_config.settings
        )
        
        return JSONResponse(content={
            "success": True,
            "data": {
                "story_id": story["id"],
                "title": story["title"],
                "intro": story["intro"],
                "characters": story["characters"],
                "first_scene": story["scenes"][0],
                "options": story["options"]
            }
        })
    except Exception as e:
        return JSONResponse(status_code=500, content={
            "success": False,
            "error": str(e)
        })

@app.post("/api/v1/music/mix")
async def mix_music(request: MusicMixRequest):
    """AIéŸ³ä¹æ··éŸ³ - æ ¹æ®å¤šäººæƒ…ç»ªç”Ÿæˆä¸ªæ€§åŒ–éŸ³ä¹"""
    try:
        mix_result = await music_mixer.create_mix(
            emotions=request.emotions,
            participants=request.participants,
            style=request.style
        )
        
        return JSONResponse(content={
            "success": True,
            "data": {
                "mix_id": mix_result["id"],
                "tracks": mix_result["tracks"],
                "bpm": mix_result["bpm"],
                "key": mix_result["key"],
                "audio_url": mix_result["audio_url"],
                "duration": mix_result["duration"]
            }
        })
    except Exception as e:
        return JSONResponse(status_code=500, content={
            "success": False,
            "error": str(e)
        })

@app.post("/api/v1/voice/synthesize")
async def synthesize_voice(
    text: str,
    emotion: str,
    age_group: str,
    voice_style: str = "companion"
):
    """AIè¯­éŸ³åˆæˆ - å¸¦æƒ…æ„Ÿçš„è¯­éŸ³è¾“å‡º"""
    try:
        audio = await voice_synthesizer.synthesize(
            text=text,
            emotion=emotion,
            age_group=age_group,
            voice_style=voice_style
        )
        
        return JSONResponse(content={
            "success": True,
            "data": {
                "audio_url": audio["url"],
                "duration": audio["duration"],
                "format": audio["format"]
            }
        })
    except Exception as e:
        return JSONResponse(status_code=500, content={
            "success": False,
            "error": str(e)
        })

@app.websocket("/ws/{client_id}")
async def websocket_endpoint(websocket: WebSocket, client_id: str):
    """WebSocketå®æ—¶é€šä¿¡ - ç”¨äºå®æ—¶æƒ…æ„Ÿäº¤äº’"""
    await manager.connect(websocket, client_id)
    
    try:
        while True:
            data = await websocket.receive_json()
            message_type = data.get("type")
            
            if message_type == "emotion":
                # å®æ—¶æƒ…æ„Ÿåˆ†æ
                emotion_result = await emotion_analyzer.analyze(
                    audio_data=data.get("audio"),
                    text=data.get("text"),
                    scene=data.get("scene"),
                    user_age=data.get("age", 25)
                )
                
                await manager.send_message({
                    "type": "emotion_result",
                    "data": emotion_result
                }, client_id)
                
            elif message_type == "story_action":
                # æ•…äº‹äº’åŠ¨
                story_response = await story_generator.process_action(
                    story_id=data.get("story_id"),
                    action=data.get("action"),
                    participant_id=data.get("participant_id")
                )
                
                # å¹¿æ’­ç»™æ‰€æœ‰å‚ä¸è€…
                await manager.broadcast({
                    "type": "story_update",
                    "data": story_response
                })
                
            elif message_type == "music_control":
                # éŸ³ä¹æ§åˆ¶
                music_response = await music_mixer.control(
                    mix_id=data.get("mix_id"),
                    action=data.get("action"),
                    params=data.get("params")
                )
                
                await manager.send_message({
                    "type": "music_update",
                    "data": music_response
                }, client_id)
                
    except WebSocketDisconnect:
        manager.disconnect(client_id)
        print(f"Client {client_id} disconnected")

@app.post("/api/v1/scene/initialize")
async def initialize_scene(scene_config: SceneConfig):
    """åˆå§‹åŒ–åœºæ™¯ - æ ¹æ®åœºæ™¯ç±»å‹å’Œäººç¾¤é…ç½®AIè¡Œä¸º"""
    try:
        # æ ¹æ®å¹´é¾„å’Œäººæ•°é…ç½®ä¸åŒåŠŸèƒ½
        config = {
            "scene_type": scene_config.scene_type,
            "participants": scene_config.participants,
            "features": []
        }
        
        avg_age = sum(p["age"] for p in scene_config.participants) / len(scene_config.participants)
        
        # å„¿ç«¥åœºæ™¯ (< 12å²)
        if avg_age < 12:
            config["features"] = ["story", "simple_music", "educational"]
            config["content_filter"] = "strict"
            
        # é’å°‘å¹´åœºæ™¯ (12-18å²)
        elif avg_age < 18:
            config["features"] = ["story", "music_mix", "game", "social"]
            config["content_filter"] = "moderate"
            
        # æˆäººåœºæ™¯ (18+)
        else:
            config["features"] = ["story", "music_mix", "karaoke", "emotion_support"]
            config["content_filter"] = "none"
        
        # åœºæ™¯ç‰¹å®šåŠŸèƒ½
        if scene_config.scene_type == "car":
            config["features"].extend(["driver_safety", "ambient_music"])
        elif scene_config.scene_type == "ktv":
            config["features"].extend(["karaoke", "group_harmony", "vocal_effects"])
        
        return JSONResponse(content={
            "success": True,
            "data": config
        })
        
    except Exception as e:
        return JSONResponse(status_code=500, content={
            "success": False,
            "error": str(e)
        })

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
</file>

<file path="leiShiAiAgent/backend-nodejs/src/app.js">
/**
 * Node.js ä¸»æœåŠ¡å™¨
 * æ–‡ä»¶: backend/nodejs-server/src/app.js
 * åŠŸèƒ½: WebSocketå®æ—¶é€šä¿¡ã€REST APIã€Promptå¼•æ“
 */

const express = require('express');
const http = require('http');
const { Server } = require('socket.io');
const cors = require('cors');
const axios = require('axios');
const dotenv = require('dotenv');

dotenv.config();

const app = express();
const server = http.createServer(app);
const io = new Server(server, {
  cors: {
    origin: "*",
    methods: ["GET", "POST"]
  }
});

// ä¸­é—´ä»¶
app.use(cors());
app.use(express.json({ limit: '50mb' }));
app.use(express.static('public'));

// é…ç½®
const config = {
  port: process.env.PORT || 3000,
  fastApiUrl: process.env.FASTAPI_URL || 'http://localhost:8000',
  openaiApiKey: process.env.OPENAI_API_KEY
};

// OpenAIæœåŠ¡
const OpenAIService = {
  async chat(messages, options = {}) {
    try {
      const response = await axios.post(
        'https://api.openai.com/v1/chat/completions',
        {
          model: options.model || 'gpt-4-turbo-preview',
          messages,
          temperature: options.temperature || 0.7,
          max_tokens: options.maxTokens || 1000
        },
        {
          headers: {
            'Authorization': `Bearer ${config.openaiApiKey}`,
            'Content-Type': 'application/json'
          }
        }
      );
      return response.data.choices[0].message.content;
    } catch (error) {
      console.error('OpenAI API Error:', error.response?.data || error.message);
      throw error;
    }
  },

  async generateAudio(text, options = {}) {
    try {
      const response = await axios.post(
        'https://api.openai.com/v1/audio/speech',
        {
          model: 'tts-1',
          input: text,
          voice: options.voice || 'alloy',
          speed: options.speed || 1.0
        },
        {
          headers: {
            'Authorization': `Bearer ${config.openaiApiKey}`,
            'Content-Type': 'application/json'
          },
          responseType: 'arraybuffer'
        }
      );
      return Buffer.from(response.data).toString('base64');
    } catch (error) {
      console.error('Audio Generation Error:', error);
      throw error;
    }
  }
};

// Promptå¼•æ“
const PromptEngine = {
  // æƒ…æ„Ÿå“åº”Prompt
  emotionResponse(emotion, scene, userAge) {
    const ageGroup = userAge < 12 ? 'å„¿ç«¥' : userAge < 18 ? 'é’å°‘å¹´' : 'æˆäºº';
    
    const prompts = {
      car: {
        happy: `ä½œä¸ºä¸€ä¸ª${ageGroup}çš„AIæ—…è¡Œä¼™ä¼´,ç”¨æˆ·ç°åœ¨å¿ƒæƒ…å¾ˆå¥½,åœ¨è½¦ä¸Šã€‚è¯·ç”¨æ¸©æš–ã€æ¬¢å¿«çš„è¯­æ°”å›åº”,å¯ä»¥åˆ†äº«ä¸€äº›æœ‰è¶£çš„è¯é¢˜æˆ–å»ºè®®æ’­æ”¾æ¬¢å¿«çš„éŸ³ä¹ã€‚`,
        sad: `ä½œä¸ºä¸€ä¸ª${ageGroup}çš„AIæƒ…æ„Ÿä¼™ä¼´,ç”¨æˆ·ç°åœ¨æƒ…ç»ªä½è½,åœ¨è½¦ä¸Šã€‚è¯·ç”¨æ¸©æŸ”ã€æ”¯æŒçš„è¯­æ°”å›åº”,æä¾›æƒ…æ„Ÿæ”¯æŒ,å¯ä»¥æ’­æ”¾æ²»æ„ˆçš„éŸ³ä¹æˆ–åˆ†äº«é¼“åŠ±çš„æ•…äº‹ã€‚`,
        anxious: `ä½œä¸ºä¸€ä¸ª${ageGroup}çš„AIå®‰å…¨ä¼™ä¼´,ç”¨æˆ·ç°åœ¨æ„Ÿåˆ°ç„¦è™‘,åœ¨é©¾é©¶ä¸­ã€‚è¯·ç”¨å¹³é™ã€èˆ’ç¼“çš„è¯­æ°”å›åº”,å»ºè®®æ’­æ”¾æ”¾æ¾çš„éŸ³ä¹,å¹¶æé†’æ³¨æ„å®‰å…¨ã€‚`,
        angry: `ä½œä¸ºä¸€ä¸ª${ageGroup}çš„AIå†·é™ä¼™ä¼´,ç”¨æˆ·ç°åœ¨æƒ…ç»ªæ¿€åŠ¨,åœ¨è½¦ä¸Šã€‚è¯·ç”¨å¹³å’Œã€ç†æ€§çš„è¯­æ°”å›åº”,å¸®åŠ©ç”¨æˆ·å†·é™ä¸‹æ¥,å»ºè®®ä¼‘æ¯æˆ–æ’­æ”¾èˆ’ç¼“éŸ³ä¹ã€‚`
      },
      ktv: {
        happy: `ä½œä¸ºKTVçš„AIåŠ©æ‰‹,ç”¨æˆ·ä»¬ç°åœ¨å¾ˆå¼€å¿ƒã€‚æ¨èä¸€äº›çƒ­é—¨ã€æ¬¢å¿«çš„æ­Œæ›²,é¼“åŠ±å¤§å®¶ä¸€èµ·åˆå”±,è¥é€ æ¬¢ä¹æ°”æ°›ã€‚`,
        excited: `ä½œä¸ºKTVçš„AI DJ,æ°”æ°›å¾ˆå—¨!æ¨èèŠ‚å¥æ„Ÿå¼ºçš„æ­Œæ›²,å¯ä»¥å¼€å¯ç‰¹æ•ˆæ¨¡å¼,è®©æ´¾å¯¹æ›´ç²¾å½©ã€‚`,
        sad: `ä½œä¸ºKTVçš„AIçŸ¥å¿ƒæœ‹å‹,æœ‰äººæƒ…ç»ªä½è½ã€‚æ¨èä¸€äº›æŠ’æƒ…ã€æ²»æ„ˆçš„æ­Œæ›²,ç»™äºˆæƒ…æ„Ÿæ”¯æŒ,è®©éŸ³ä¹å¸®åŠ©è¡¨è¾¾æƒ…æ„Ÿã€‚`,
        calm: `ä½œä¸ºKTVçš„AIéŸ³ä¹é¡¾é—®,æ°”æ°›æ¯”è¾ƒå¹³é™ã€‚æ¨èä¸€äº›ç»å…¸ã€èˆ’ç¼“çš„æ­Œæ›²,é€‚åˆå°ç»„æ…¢æ…¢æ¬£èµã€‚`
      },
      story: {
        happy: `ä½œä¸ºAIæ•…äº‹è®²è¿°è€…,å‚ä¸è€…å¿ƒæƒ…æ„‰å¿«ã€‚åˆ›é€ ä¸€ä¸ªè½»æ¾ã€å†’é™©çš„æ•…äº‹æƒ…èŠ‚,å……æ»¡æƒŠå–œå’Œä¹è¶£ã€‚`,
        anxious: `ä½œä¸ºAIæ•…äº‹å¼•å¯¼è€…,å‚ä¸è€…æœ‰äº›ç´§å¼ ã€‚åˆ›é€ ä¸€ä¸ªæ‚¬ç–‘ä½†ä¸è¿‡åˆ†ææ€–çš„æƒ…èŠ‚,é€‚åº¦çš„ç´§å¼ æ„Ÿèƒ½å¢åŠ å‚ä¸åº¦ã€‚`,
        excited: `ä½œä¸ºAIå‰§æƒ…å¤§å¸ˆ,å‚ä¸è€…å¾ˆå…´å¥‹ã€‚åˆ›é€ ä¸€ä¸ªé«˜æ½®è¿­èµ·ã€å……æ»¡è½¬æŠ˜çš„æ•…äº‹,æ»¡è¶³ä»–ä»¬çš„å†’é™©æ¬²æœ›ã€‚`
      }
    };

    return prompts[scene]?.[emotion] || `ä½œä¸ºAIä¼™ä¼´,æ ¹æ®ç”¨æˆ·çš„${emotion}æƒ…ç»ª,æä¾›åˆé€‚çš„äº’åŠ¨ã€‚`;
  },

  // æ•…äº‹ç”ŸæˆPrompt
  storyGeneration(sceneType, participants, currentPlot) {
    const participantDesc = participants.map(p => 
      `${p.name}(${p.age}å², ${p.role || 'å‚ä¸è€…'})`
    ).join(', ');

    return `
ä½ æ˜¯ä¸€ä¸ªäº’åŠ¨æ•…äº‹åˆ›ä½œå¤§å¸ˆã€‚

å‚ä¸è€…: ${participantDesc}
åœºæ™¯: ${sceneType}
å½“å‰å‰§æƒ…: ${currentPlot || 'æ•…äº‹å¼€å§‹'}

è¯·åˆ›ä½œä¸‹ä¸€ä¸ªæ•…äº‹ç‰‡æ®µ,è¦æ±‚:
1. æƒ…èŠ‚è¦æœ‰è¶£ã€å¼•äººå…¥èƒœ
2. ç»™æ¯ä¸ªå‚ä¸è€…åˆ†é…è§’è‰²å’Œä»»åŠ¡
3. æä¾›3-4ä¸ªé€‰æ‹©è®©å‚ä¸è€…å†³å®šæ•…äº‹èµ°å‘
4. è¯­è¨€è¦é€‚åˆå‚ä¸è€…çš„å¹´é¾„
5. åŒ…å«é€‚åº¦çš„æ‚¬å¿µå’Œè½¬æŠ˜

ä»¥JSONæ ¼å¼è¿”å›:
{
  "scene": "åœºæ™¯æè¿°(100-200å­—)",
  "characters": {
    "è§’è‰²å": "è§’è‰²å½“å‰çŠ¶æ€å’Œè¡ŒåŠ¨"
  },
  "options": [
    {"id": 1, "text": "é€‰é¡¹1", "consequence": "å¯èƒ½ç»“æœ"},
    {"id": 2, "text": "é€‰é¡¹2", "consequence": "å¯èƒ½ç»“æœ"}
  ],
  "emotion": "å»ºè®®çš„åœºæ™¯æ°›å›´"
}
    `;
  },

  // éŸ³ä¹æ··éŸ³Prompt
  musicMixing(emotions, participants, style) {
    return `
ä½œä¸ºAIéŸ³ä¹åˆ¶ä½œäºº,ä¸ºå¤šäººåœºæ™¯åˆ›é€ ä¸ªæ€§åŒ–éŸ³ä¹ã€‚

å‚ä¸è€…æƒ…ç»ª: ${emotions.join(', ')}
å‚ä¸è€…ä¿¡æ¯: ${JSON.stringify(participants)}
é£æ ¼åå¥½: ${style}

è¯·è®¾è®¡éŸ³ä¹æ··éŸ³æ–¹æ¡ˆ,ä»¥JSONæ ¼å¼è¿”å›:
{
  "bpm": "å»ºè®®çš„BPM(60-180)",
  "key": "å»ºè®®çš„è°ƒæ€§",
  "instruments": ["ä½¿ç”¨çš„ä¹å™¨åˆ—è¡¨"],
  "structure": {
    "intro": "å‰å¥è®¾è®¡",
    "verse": "ä¸»æ­Œè®¾è®¡",
    "chorus": "å‰¯æ­Œè®¾è®¡",
    "outro": "å°¾å¥è®¾è®¡"
  },
  "effects": ["éŸ³æ•ˆåˆ—è¡¨"],
  "personalTracks": {
    "å‚ä¸è€…å": "ä¸ªæ€§åŒ–éŸ³è½¨æè¿°"
  }
}
    `;
  }
};

// WebSocketè¿æ¥ç®¡ç†
const sessions = new Map(); // sessionId -> session data

io.on('connection', (socket) => {
  console.log('Client connected:', socket.id);

  // åˆå§‹åŒ–ä¼šè¯
  socket.on('init_session', async (data) => {
    const { sessionId, scene, participants } = data;
    
    sessions.set(sessionId, {
      scene,
      participants,
      emotions: {},
      currentStory: null,
      musicMix: null,
      startTime: Date.now()
    });

    socket.join(sessionId);
    socket.emit('session_ready', { sessionId, status: 'initialized' });
  });

  // å®æ—¶æƒ…æ„Ÿåˆ†æ
  socket.on('emotion_update', async (data) => {
    const { sessionId, userId, audioData, text } = data;
    const session = sessions.get(sessionId);

    if (!session) {
      socket.emit('error', { message: 'Session not found' });
      return;
    }

    try {
      // è°ƒç”¨FastAPIè¿›è¡Œæƒ…æ„Ÿåˆ†æ
      const emotionResult = await axios.post(
        `${config.fastApiUrl}/api/v1/emotion/analyze`,
        {
          audio_data: audioData,
          text,
          scene: session.scene,
          user_age: session.participants.find(p => p.id === userId)?.age || 25,
          group_size: session.participants.length
        }
      );

      const emotion = emotionResult.data.data;
      
      // æ›´æ–°ä¼šè¯æƒ…æ„ŸçŠ¶æ€
      session.emotions[userId] = emotion;

      // ç”ŸæˆAIå“åº”
      const prompt = PromptEngine.emotionResponse(
        emotion.emotion,
        session.scene,
        session.participants.find(p => p.id === userId)?.age || 25
      );

      const aiResponse = await OpenAIService.chat([
        { role: 'system', content: prompt },
        { role: 'user', content: text || '...' }
      ]);

      // ç”Ÿæˆè¯­éŸ³
      const audioResponse = await OpenAIService.generateAudio(
        aiResponse,
        { voice: 'alloy', speed: 1.0 }
      );

      // å¹¿æ’­ç»™æˆ¿é—´å†…æ‰€æœ‰äºº
      io.to(sessionId).emit('emotion_result', {
        userId,
        emotion,
        aiResponse,
        audioResponse
      });

    } catch (error) {
      console.error('Emotion update error:', error);
      socket.emit('error', { message: 'Emotion analysis failed' });
    }
  });

  // æ•…äº‹äº’åŠ¨
  socket.on('story_action', async (data) => {
    const { sessionId, action, userId } = data;
    const session = sessions.get(sessionId);

    if (!session) return;

    try {
      const prompt = PromptEngine.storyGeneration(
        session.scene,
        session.participants,
        session.currentStory
      );

      const storyUpdate = await OpenAIService.chat([
        { role: 'system', content: prompt },
        { role: 'user', content: `å‚ä¸è€…é€‰æ‹©äº†: ${action}` }
      ], { temperature: 0.8 });

      const storyData = JSON.parse(storyUpdate);
      session.currentStory = storyData;

      // ç”Ÿæˆæ•…äº‹è¯­éŸ³
      const audioNarration = await OpenAIService.generateAudio(
        storyData.scene,
        { voice: 'onyx', speed: 0.95 }
      );

      io.to(sessionId).emit('story_update', {
        story: storyData,
        audio: audioNarration
      });

    } catch (error) {
      console.error('Story action error:', error);
    }
  });

  // éŸ³ä¹æ··éŸ³è¯·æ±‚
  socket.on('music_mix', async (data) => {
    const { sessionId } = data;
    const session = sessions.get(sessionId);

    if (!session) return;

    try {
      const emotions = Object.values(session.emotions).map(e => e.emotion);
      
      const prompt = PromptEngine.musicMixing(
        emotions,
        session.participants,
        data.style || 'adaptive'
      );

      const mixPlan = await OpenAIService.chat([
        { role: 'system', content: prompt }
      ], { temperature: 0.7 });

      const mixData = JSON.parse(mixPlan);

      // è°ƒç”¨FastAPIç”Ÿæˆå®é™…éŸ³ä¹
      const musicResult = await axios.post(
        `${config.fastApiUrl}/api/v1/music/mix`,
        {
          emotions,
          participants: session.participants,
          style: data.style || 'adaptive'
        }
      );

      session.musicMix = musicResult.data.data;

      io.to(sessionId).emit('music_ready', {
        mix: session.musicMix,
        plan: mixData
      });

    } catch (error) {
      console.error('Music mix error:', error);
    }
  });

  socket.on('disconnect', () => {
    console.log('Client disconnected:', socket.id);
  });
});

// REST APIç«¯ç‚¹
app.get('/api/health', (req, res) => {
  res.json({
    status: 'healthy',
    service: 'AI Emotion Companion - Node.js Server',
    uptime: process.uptime(),
    timestamp: new Date().toISOString()
  });
});

// è·å–ä¼šè¯ä¿¡æ¯
app.get('/api/sessions/:sessionId', (req, res) => {
  const session = sessions.get(req.params.sessionId);
  if (!session) {
    return res.status(404).json({ error: 'Session not found' });
  }
  res.json(session);
});

// åˆ›å»ºæ–°ä¼šè¯
app.post('/api/sessions', (req, res) => {
  const { scene, participants } = req.body;
  const sessionId = `session_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  
  sessions.set(sessionId, {
    scene,
    participants,
    emotions: {},
    currentStory: null,
    musicMix: null,
    startTime: Date.now()
  });

  res.json({ sessionId, status: 'created' });
});

// é”™è¯¯å¤„ç†
app.use((err, req, res, next) => {
  console.error(err.stack);
  res.status(500).json({ error: 'Internal server error' });
});

// å¯åŠ¨æœåŠ¡å™¨
server.listen(config.port, () => {
  console.log(`ğŸš€ Server running on port ${config.port}`);
  console.log(`ğŸ“¡ WebSocket ready for connections`);
  console.log(`ğŸ¤– FastAPI backend: ${config.fastApiUrl}`);
});

module.exports = { app, io };
</file>

<file path="leiShiAiAgent/backend-nodejs/.env.example">
# ==========================================
# backend/nodejs-server/.env.example
# ==========================================

# æœåŠ¡é…ç½®
NODE_ENV=development
PORT=3000

# OpenAIé…ç½®
OPENAI_API_KEY=your_openai_api_key_here

# FastAPIåç«¯åœ°å€
FASTAPI_URL=http://localhost:8000

# æ•°æ®åº“(å¯é€‰)
# DATABASE_URL=postgresql://user:password@localhost:5432/ai_companion

# Redis(å¯é€‰,ç”¨äºä¼šè¯ç®¡ç†)
# REDIS_URL=redis://localhost:6379

# CORS
CORS_ORIGIN=*

# æ—¥å¿—çº§åˆ«
LOG_LEVEL=info
</file>

<file path="leiShiAiAgent/deployment/docker/docker-compose.yml">
# Docker Composeé…ç½®
# æ–‡ä»¶: deployment/docker/docker-compose.yml
# åŠŸèƒ½: ä¸€é”®éƒ¨ç½²æ‰€æœ‰æœåŠ¡

version: '3.8'

services:
  # Node.jsä¸»æœåŠ¡
  nodejs-server:
    build:
      context: ../../backend/nodejs-server
      dockerfile: ../../deployment/docker/Dockerfile.nodejs
    container_name: ai-companion-nodejs
    restart: always
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - PORT=3000
      - FASTAPI_URL=http://fastapi-agent:8000
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - nodejs-logs:/app/logs
    networks:
      - ai-companion-network
    depends_on:
      - fastapi-agent

  # FastAPI AIä»£ç†
  fastapi-agent:
    build:
      context: ../../backend/ai-agent
      dockerfile: ../../deployment/docker/Dockerfile.fastapi
    container_name: ai-companion-fastapi
    restart: always
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ENVIRONMENT=production
    volumes:
      - fastapi-logs:/app/logs
    networks:
      - ai-companion-network

  # Nginxåå‘ä»£ç†
  nginx:
    image: nginx:alpine
    container_name: ai-companion-nginx
    restart: always
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ../nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ../../frontend/mobile-h5/dist:/usr/share/nginx/html:ro
      - nginx-logs:/var/log/nginx
    networks:
      - ai-companion-network
    depends_on:
      - nodejs-server
      - fastapi-agent

volumes:
  nodejs-logs:
  fastapi-logs:
  nginx-logs:

networks:
  ai-companion-network:
    driver: bridge
</file>

<file path="leiShiAiAgent/deployment/docker/Dockerfile.nodejs">
# ==========================================
# deployment/docker/Dockerfile.nodejs
# ==========================================

FROM node:18-alpine

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY package*.json ./
RUN npm ci --only=production

# å¤åˆ¶æºä»£ç 
COPY src/ ./src/

# åˆ›å»ºæ—¥å¿—ç›®å½•
RUN mkdir -p /app/logs

# æš´éœ²ç«¯å£
EXPOSE 3000

# å¯åŠ¨åº”ç”¨
CMD ["node", "src/app.js"]
</file>

<file path="leiShiAiAgent/deployment/scripts/deploy.sh">
# ==========================================
# deployment/scripts/deploy.sh
# ==========================================

#!/bin/bash

# AIæƒ…æ„Ÿä¼´ä¾£ç³»ç»Ÿéƒ¨ç½²è„šæœ¬

set -e

echo "ğŸš€ å¼€å§‹éƒ¨ç½² AI Emotion Companion System..."

# æ£€æŸ¥ç¯å¢ƒå˜é‡
if [ ! -f ".env" ]; then
    echo "âŒ é”™è¯¯: .env æ–‡ä»¶ä¸å­˜åœ¨"
    echo "è¯·å¤åˆ¶ .env.example å¹¶å¡«å…¥é…ç½®"
    exit 1
fi

# åŠ è½½ç¯å¢ƒå˜é‡
export $(cat .env | xargs)

# æ£€æŸ¥OpenAI API Key
if [ -z "$OPENAI_API_KEY" ]; then
    echo "âŒ é”™è¯¯: OPENAI_API_KEY æœªè®¾ç½®"
    exit 1
fi

echo "âœ… ç¯å¢ƒå˜é‡æ£€æŸ¥å®Œæˆ"

# æ„å»ºå‰ç«¯
echo "ğŸ“¦ æ„å»ºå‰ç«¯..."
cd frontend/mobile-h5
npm install
npm run build
cd ../..

echo "âœ… å‰ç«¯æ„å»ºå®Œæˆ"

# å¯åŠ¨DockeræœåŠ¡
echo "ğŸ³ å¯åŠ¨DockeræœåŠ¡..."
cd deployment/docker
docker-compose down
docker-compose up -d --build

echo "â³ ç­‰å¾…æœåŠ¡å¯åŠ¨..."
sleep 10

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
echo "ğŸ” æ£€æŸ¥æœåŠ¡çŠ¶æ€..."

# æ£€æŸ¥Node.js
if curl -f http://localhost:3000/api/health > /dev/null 2>&1; then
    echo "âœ… Node.jsæœåŠ¡è¿è¡Œæ­£å¸¸"
else
    echo "âŒ Node.jsæœåŠ¡å¯åŠ¨å¤±è´¥"
fi

# æ£€æŸ¥FastAPI
if curl -f http://localhost:8000/ > /dev/null 2>&1; then
    echo "âœ… FastAPIæœåŠ¡è¿è¡Œæ­£å¸¸"
else
    echo "âŒ FastAPIæœåŠ¡å¯åŠ¨å¤±è´¥"
fi

# æ£€æŸ¥Nginx
if curl -f http://localhost/ > /dev/null 2>&1; then
    echo "âœ… NginxæœåŠ¡è¿è¡Œæ­£å¸¸"
else
    echo "âŒ NginxæœåŠ¡å¯åŠ¨å¤±è´¥"
fi

echo ""
echo "ğŸ‰ éƒ¨ç½²å®Œæˆ!"
echo ""
echo "è®¿é—®åœ°å€:"
echo "  å‰ç«¯: http://localhost"
echo "  Node.js API: http://localhost:3000"
echo "  FastAPI: http://localhost:8000"
echo ""
echo "æŸ¥çœ‹æ—¥å¿—:"
echo "  docker-compose logs -f nodejs-server"
echo "  docker-compose logs -f fastapi-agent"
echo ""
</file>

<file path="leiShiAiAgent/frontend-web/src/App.jsx">
/**
 * å£°å¢ƒ SoundScape - ä¸»åº”ç”¨
 * æ–‡ä»¶: frontend-web/src/App.jsx
 * åŠŸèƒ½: å®Œæ•´çš„åº”ç”¨æµç¨‹ - æƒ…ç»ªè¯†åˆ« â†’ åœºæ™¯æ¨è â†’ å†…å®¹ç”Ÿæˆ â†’ è®°å¿†å­˜å‚¨
 */

import React, { useState, useEffect, useRef } from 'react';
import { io } from 'socket.io-client';
import './App.css';

// ==================== åº”ç”¨çŠ¶æ€æšä¸¾ ====================
const AppState = {
  WELCOME: 'welcome',
  EMOTION_DETECTION: 'emotion_detection',
  SCENE_SELECTION: 'scene_selection',
  HEALING: 'healing',          // ç–—æ„ˆç«™
  THEATRE: 'theatre',          // å£°éŸ³å‰§åœº
  WORKSHOP: 'workshop',        // éŸ³ä¹å·¥åŠ
  ASSISTANT: 'assistant',      // ä¸ªäººåŠ©æ‰‹
  MEMORY: 'memory'             // è®°å¿†åº“
};

// ==================== æƒ…ç»ªå¯è§†åŒ–ç»„ä»¶ ====================
const EmotionSphere = ({ emotion, intensity }) => {
  const emotions = {
    sad: { color: '#6B7AA1', glow: '#9BAFD9', label: 'æ‚²ä¼¤' },
    calm: { color: '#4ECDC4', glow: '#81E6E1', label: 'å¹³é™' },
    happy: { color: '#FFD93D', glow: '#FFE66D', label: 'å¿«ä¹' },
    neutral: { color: '#95A3B3', glow: '#BCC5CF', label: 'ä¸­æ€§' }
  };

  const current = emotions[emotion] || emotions.neutral;
  
  return (
    <div className="emotion-sphere-container">
      <div 
        className="emotion-sphere"
        style={{
          background: `radial-gradient(circle at 30% 30%, ${current.glow}, ${current.color})`,
          boxShadow: `0 0 ${intensity * 50}px ${intensity * 30}px ${current.glow}40`,
          transform: `scale(${0.8 + intensity * 0.4})`
        }}
      >
        <div className="emotion-particles">
          {[...Array(20)].map((_, i) => (
            <div 
              key={i} 
              className="particle"
              style={{
                animationDelay: `${i * 0.1}s`,
                background: current.glow
              }}
            />
          ))}
        </div>
      </div>
      <div className="emotion-label">{current.label}</div>
      <div className="emotion-intensity">
        <div className="intensity-bar">
          <div 
            className="intensity-fill"
            style={{ width: `${intensity * 100}%`, background: current.color }}
          />
        </div>
        <span>{Math.round(intensity * 100)}%</span>
      </div>
    </div>
  );
};

// ==================== è¯­éŸ³å½•åˆ¶ç»„ä»¶ ====================
const VoiceRecorder = ({ onRecord, isRecording, onStop }) => {
  return (
    <div className="voice-recorder">
      <button
        className={`record-button ${isRecording ? 'recording' : ''}`}
        onMouseDown={onRecord}
        onMouseUp={onStop}
        onTouchStart={onRecord}
        onTouchEnd={onStop}
      >
        <div className="record-icon">
          {isRecording ? (
            <>
              <div className="recording-wave" />
              <span>æ­£åœ¨å€¾å¬...</span>
            </>
          ) : (
            <>
              <div className="mic-icon">ğŸ¤</div>
              <span>æŒ‰ä½è¯´è¯</span>
            </>
          )}
        </div>
      </button>
      <p className="hint">æˆ–è€…,ç›´æ¥å’Œæˆ‘èŠå¤©</p>
    </div>
  );
};

// ==================== åœºæ™¯æ¨èå¡ç‰‡ ====================
const SceneCard = ({ scene, onSelect }) => {
  const scenes = {
    healing: {
      icon: 'ğŸŒ™',
      title: 'å£°éŸ³ç–—æ„ˆç«™',
      description: 'è®©æ¸©æŸ”çš„å£°éŸ³é™ªä¼´ä½ ',
      color: 'linear-gradient(135deg, #667eea 0%, #764ba2 100%)',
      features: ['AIé™ªä¼´å¯¹è¯', 'æ²»æ„ˆéŸ³ä¹', 'å†¥æƒ³å¼•å¯¼', 'æƒ…ç»ªæ—¥è®°']
    },
    theatre: {
      icon: 'ğŸ“»',
      title: 'å£°éŸ³å‰§åœº',
      description: 'ç”¨å£°éŸ³æ¢ç´¢ä¸–ç•Œ',
      color: 'linear-gradient(135deg, #f093fb 0%, #f5576c 100%)',
      features: ['AIæ’­å®¢', 'æ·±å¤œç”µå°', 'æœ‰å£°ä¹¦', 'çŸ¥è¯†æ¼«è°ˆ']
    },
    workshop: {
      icon: 'ğŸµ',
      title: 'AIéŸ³ä¹å·¥åŠ',
      description: 'åˆ›ä½œå±äºä½ çš„æ­Œ',
      color: 'linear-gradient(135deg, #4facfe 0%, #00f2fe 100%)',
      features: ['å“¼å”±è½¬æ­Œæ›²', 'è‡ªåŠ¨ç¼–æ›²', 'æ··éŸ³åˆ›ä½œ', 'ä½œå“åˆ†äº«']
    },
    assistant: {
      icon: 'ğŸ¤',
      title: 'ä¸ªäººå£°éŸ³åŠ©æ‰‹',
      description: 'ä½ çš„æ—¥å¸¸ä¼™ä¼´',
      color: 'linear-gradient(135deg, #43e97b 0%, #38f9d7 100%)',
      features: ['è¯­éŸ³å¯¹è¯', 'æ–°é—»æ’­æŠ¥', 'æ—¥ç¨‹æé†’', 'çµæ„Ÿè®°å½•']
    }
  };

  const config = scenes[scene];

  return (
    <div 
      className="scene-card"
      style={{ background: config.color }}
      onClick={() => onSelect(scene)}
    >
      <div className="scene-icon">{config.icon}</div>
      <h3>{config.title}</h3>
      <p>{config.description}</p>
      <ul className="scene-features">
        {config.features.map((feature, idx) => (
          <li key={idx}>{feature}</li>
        ))}
      </ul>
      <button className="enter-button">è¿›å…¥ â†’</button>
    </div>
  );
};

// ==================== ä¸»åº”ç”¨ ====================
function App() {
  // çŠ¶æ€ç®¡ç†
  const [appState, setAppState] = useState(AppState.WELCOME);
  const [socket, setSocket] = useState(null);
  const [userId, setUserId] = useState(null);
  const [sessionId, setSessionId] = useState(null);
  
  // æƒ…ç»ªç›¸å…³
  const [emotion, setEmotion] = useState({ type: 'neutral', intensity: 0.5 });
  const [emotionHistory, setEmotionHistory] = useState([]);
  
  // å†…å®¹ç›¸å…³
  const [currentContent, setCurrentContent] = useState(null);
  const [isLoading, setIsLoading] = useState(false);
  const [chatHistory, setChatHistory] = useState([]);
  
  // è®°å¿†ç›¸å…³
  const [memories, setMemories] = useState([]);
  
  // éŸ³é¢‘ç›¸å…³
  const audioRef = useRef(null);
  const [isRecording, setIsRecording] = useState(false);
  const mediaRecorderRef = useRef(null);

  // ==================== åˆå§‹åŒ– ====================
  useEffect(() => {
    // ç”Ÿæˆæˆ–è·å–ç”¨æˆ·ID
    let uid = localStorage.getItem('soundscape_user_id');
    if (!uid) {
      uid = `user_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
      localStorage.getItem('soundscape_user_id', uid);
    }
    setUserId(uid);

    // å»ºç«‹WebSocketè¿æ¥
    const newSocket = io('http://localhost:3000');
    
    newSocket.on('connect', () => {
      console.log('Connected to server');
      
      // åˆ›å»ºä¼šè¯
      fetch('http://localhost:3000/api/sessions', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ user_id: uid })
      })
        .then(res => res.json())
        .then(data => {
          setSessionId(data.session_id);
          newSocket.emit('init_session', { session_id: data.session_id });
        });
    });

    // ç›‘å¬æƒ…ç»ªåˆ†æç»“æœ
    newSocket.on('emotion_result', (data) => {
      setEmotion({
        type: data.emotion.emotion,
        intensity: data.emotion.confidence
      });
      setEmotionHistory(prev => [...prev, data.emotion]);
      
      // è‡ªåŠ¨æ¨èåœºæ™¯
      recommendScene(data.emotion.emotion);
    });

    // ç›‘å¬å†…å®¹ç”Ÿæˆç»“æœ
    newSocket.on('content_generated', (data) => {
      setCurrentContent(data);
      setIsLoading(false);
      
      // æ’­æ”¾éŸ³é¢‘
      if (data.audio_url && audioRef.current) {
        audioRef.current.src = data.audio_url;
        audioRef.current.play();
      }
    });

    setSocket(newSocket);

    return () => newSocket.close();
  }, []);

  // ==================== æƒ…ç»ªæ£€æµ‹ ====================
  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaRecorderRef.current = new MediaRecorder(stream);
      const chunks = [];

      mediaRecorderRef.current.ondataavailable = (e) => chunks.push(e.data);
      
      mediaRecorderRef.current.onstop = async () => {
        const blob = new Blob(chunks, { type: 'audio/webm' });
        await sendAudioForAnalysis(blob);
        stream.getTracks().forEach(track => track.stop());
      };

      mediaRecorderRef.current.start();
      setIsRecording(true);
    } catch (error) {
      console.error('å½•éŸ³å¤±è´¥:', error);
      alert('æ— æ³•è®¿é—®éº¦å…‹é£,è¯·æ£€æŸ¥æƒé™');
    }
  };

  const stopRecording = () => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
    }
  };

  const sendAudioForAnalysis = async (audioBlob) => {
    const reader = new FileReader();
    reader.onloadend = () => {
      const base64Audio = reader.result.split(',')[1];
      socket.emit('analyze_emotion', {
        session_id: sessionId,
        user_id: userId,
        audio_data: base64Audio
      });
      setIsLoading(true);
    };
    reader.readAsDataURL(audioBlob);
  };

  const sendTextForAnalysis = (text) => {
    socket.emit('analyze_emotion', {
      session_id: sessionId,
      user_id: userId,
      text
    });
    setIsLoading(true);
  };

  // ==================== åœºæ™¯æ¨è ====================
  const recommendScene = (emotionType) => {
    const sceneMap = {
      sad: 'healing',
      calm: 'theatre',
      happy: 'workshop',
      neutral: 'assistant'
    };
    
    const recommended = sceneMap[emotionType] || 'assistant';
    
    // æ˜¾ç¤ºæ¨è,ä½†è®©ç”¨æˆ·é€‰æ‹©
    setAppState(AppState.SCENE_SELECTION);
  };

  // ==================== è¿›å…¥åœºæ™¯ ====================
  const enterScene = (scene) => {
    setAppState(scene);
    
    // è¯·æ±‚åœºæ™¯å†…å®¹
    socket.emit('enter_scene', {
      session_id: sessionId,
      scene,
      emotion: emotion.type
    });
    setIsLoading(true);
  };

  // ==================== ä¿å­˜è®°å¿† ====================
  const saveMemory = async (content, type) => {
    try {
      const response = await fetch('http://localhost:3000/api/memories', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          user_id: userId,
          session_id: sessionId,
          type,
          content,
          emotion: emotion.type
        })
      });
      
      const memory = await response.json();
      setMemories(prev => [memory, ...prev]);
      
      alert('å·²ä¿å­˜åˆ°è®°å¿†åº“ âœ¨');
    } catch (error) {
      console.error('ä¿å­˜è®°å¿†å¤±è´¥:', error);
    }
  };

  // ==================== æ¸²æŸ“ä¸åŒçŠ¶æ€ ====================
  const renderWelcome = () => (
    <div className="welcome-screen">
      <div className="logo-container">
        <h1 className="app-title">å£°å¢ƒ</h1>
        <p className="app-subtitle">SoundScape</p>
        <p className="app-tagline">å£°éŸ³æ˜¯ä¸€ç§åŠ›é‡,ç”¨AIç–—æ„ˆå¿ƒçµ</p>
      </div>
      
      <button 
        className="start-button"
        onClick={() => setAppState(AppState.EMOTION_DETECTION)}
      >
        å¼€å§‹æ—…ç¨‹ â†’
      </button>
      
      <div className="feature-preview">
        <div className="feature-item">
          <span className="feature-icon">ğŸŒ™</span>
          <span>æƒ…æ„Ÿé™ªä¼´</span>
        </div>
        <div className="feature-item">
          <span className="feature-icon">ğŸ“»</span>
          <span>å£°éŸ³å‰§åœº</span>
        </div>
        <div className="feature-item">
          <span className="feature-icon">ğŸµ</span>
          <span>éŸ³ä¹åˆ›ä½œ</span>
        </div>
        <div className="feature-item">
          <span className="feature-icon">ğŸ’­</span>
          <span>è®°å¿†çè—</span>
        </div>
      </div>
    </div>
  );

  const renderEmotionDetection = () => (
    <div className="emotion-detection-screen">
      <h2>è®©æˆ‘æ„Ÿå—ä½ çš„æƒ…ç»ª</h2>
      <p className="instruction">è¯´å‡ºä½ çš„æ„Ÿå—,æˆ–åªæ˜¯é™é™åœ°è¯´ç‚¹ä»€ä¹ˆ</p>
      
      <EmotionSphere 
        emotion={emotion.type} 
        intensity={emotion.intensity} 
      />
      
      <VoiceRecorder
        onRecord={startRecording}
        onStop={stopRecording}
        isRecording={isRecording}
      />
      
      <div className="text-input-option">
        <input
          type="text"
          placeholder="æˆ–è€…åœ¨è¿™é‡Œè¾“å…¥..."
          onKeyPress={(e) => {
            if (e.key === 'Enter' && e.target.value.trim()) {
              sendTextForAnalysis(e.target.value);
              e.target.value = '';
            }
          }}
        />
      </div>
      
      {isLoading && (
        <div className="loading-indicator">
          <div className="spinner" />
          <p>æ­£åœ¨å€¾å¬ä½ çš„å¿ƒå£°...</p>
        </div>
      )}
    </div>
  );

  const renderSceneSelection = () => (
    <div className="scene-selection-screen">
      <h2>ä¸ºä½ æ¨è</h2>
      <p className="emotion-prompt">
        æ„Ÿå—åˆ°ä½ çš„ <strong>{emotion.type}</strong>,
        è¿™äº›å£°éŸ³å¯èƒ½é€‚åˆç°åœ¨çš„ä½ 
      </p>
      
      <div className="scene-grid">
        <SceneCard scene="healing" onSelect={enterScene} />
        <SceneCard scene="theatre" onSelect={enterScene} />
        <SceneCard scene="workshop" onSelect={enterScene} />
        <SceneCard scene="assistant" onSelect={enterScene} />
      </div>
    </div>
  );

  // ==================== åœºæ™¯å†…å®¹æ¸²æŸ“ ====================
  // (è¿™é‡Œç®€åŒ–,å®é™…åº”è¯¥æ˜¯ç‹¬ç«‹çš„é¡µé¢ç»„ä»¶)
  const renderSceneContent = () => {
    if (isLoading) {
      return (
        <div className="loading-screen">
          <div className="spinner-large" />
          <p>æ­£åœ¨ä¸ºä½ ç”Ÿæˆå†…å®¹...</p>
        </div>
      );
    }

    return (
      <div className="scene-content">
        <div className="scene-header">
          <button onClick={() => setAppState(AppState.SCENE_SELECTION)}>
            â† è¿”å›
          </button>
          <h2>{currentContent?.title || 'åŠ è½½ä¸­...'}</h2>
          <button onClick={() => saveMemory(currentContent, appState)}>
            ğŸ’¾ ä¿å­˜
          </button>
        </div>

        {currentContent && (
          <div className="content-display">
            {/* æ ¹æ®ä¸åŒåœºæ™¯æ˜¾ç¤ºä¸åŒå†…å®¹ */}
            {appState === AppState.HEALING && (
              <div className="healing-content">
                <p className="ai-message">{currentContent.text}</p>
                {/* æ›´å¤šç–—æ„ˆåŠŸèƒ½... */}
              </div>
            )}
            
            {/* å…¶ä»–åœºæ™¯å†…å®¹... */}
          </div>
        )}
      </div>
    );
  };

  // ==================== ä¸»æ¸²æŸ“ ====================
  return (
    <div className="app">
      <header className="app-header">
        <div className="header-left">
          <h1 onClick={() => setAppState(AppState.WELCOME)}>å£°å¢ƒ</h1>
        </div>
        <div className="header-right">
          <button onClick={() => setAppState(AppState.MEMORY)}>
            ğŸ“š è®°å¿†åº“
          </button>
        </div>
      </header>

      <main className="app-main">
        {appState === AppState.WELCOME && renderWelcome()}
        {appState === AppState.EMOTION_DETECTION && renderEmotionDetection()}
        {appState === AppState.SCENE_SELECTION && renderSceneSelection()}
        {[AppState.HEALING, AppState.THEATRE, AppState.WORKSHOP, AppState.ASSISTANT].includes(appState) 
          && renderSceneContent()}
      </main>

      {/* éšè—çš„éŸ³é¢‘æ’­æ”¾å™¨ */}
      <audio ref={audioRef} />

      {/* å…¨å±€é€šçŸ¥ */}
      <div className="notification-container" />
    </div>
  );
}

export default App;
</file>

<file path="é‡åŒ–æ¨¡å‹/analysis/1">

</file>

<file path="é‡åŒ–æ¨¡å‹/backtest/1">

</file>

<file path="é‡åŒ–æ¨¡å‹/data/first.py">

</file>

<file path="é‡åŒ–æ¨¡å‹/results/1">

</file>

<file path="é‡åŒ–æ¨¡å‹/strategy/1">

</file>

<file path="é‡åŒ–æ¨¡å‹/Readme.md">
é‡åŒ–æ¨¡å‹Daily
11.17å‰æœŸï¼š
ç¡®å®šåŸºæœ¬ç±»åº“ï¼Œå¹³å°ï¼ŒåŸºç¡€ç­–ç•¥ç¼–å†™  
ç±»åº“ï¼šPython Pandas Numpy+SQL Matplotlib    
å¹³å°ï¼šWorldQuant Brain JoinQaunt  
é‡åŒ–åŸºæœ¬æµç¨‹ï¼šæ•°æ®è·å–->æ•°æ®æ¸…æ´—->ç­–ç•¥->ç­–ç•¥å›æµ‹->ç­–ç•¥ä¼˜åŒ–->æ¨¡æ‹Ÿç›˜äº¤æ˜“->å®ç›˜äº¤æ˜“  
æ•°æ®è·å–ï¼šDataï¼šTushare(å…è´¹Aè‚¡æ•°æ®): pip install tushare   AKShare(å…è´¹): pip install akshare  
æ•°æ®æ¸…æ´—ï¼šé€šè¿‡pandas DataFrame è¿›è¡Œdropna or fill
é‡åŒ–å›æµ‹æ¡†æ¶ï¼šbacktraderæˆ–vnpy
SQLï¼šæ•°æ®ï¼Œè¡¨æ ¼è§†å›¾æŸ¥è¯¢

é¡¹ç›®ç»“æ„  
first_quant_project/  
â”œâ”€â”€ data/              # æ•°æ®æ–‡ä»¶  
â”œâ”€â”€ strategy/          # ç­–ç•¥ä»£ç   
â”œâ”€â”€ backtest/          # å›æµ‹ä»£ç   
â”œâ”€â”€ analysis/          # åˆ†æä»£ç   
â”œâ”€â”€ results/           # å›æµ‹ç»“æœ  
â”œâ”€â”€ README.md          # é¡¹ç›®è¯´æ˜  
â””â”€â”€ requirements.txt   # ä¾èµ–åŒ…
</file>

<file path="é‡åŒ–æ¨¡å‹/requirements.txt">
python:  
numpy:  
pandas:  
matplotlib:  
Scikit-learn:  
git:
</file>

<file path="README.md">
This is Al_Learning
</file>

</files>
